{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducability\n",
    "torch.manual_seed(311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(64),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.Resize(64),\n",
    "        #transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder('Final_Data_Reduced/Train', data_transforms['train']),\n",
    "    'val': datasets.ImageFolder('Final_Data_Reduced/Validation', data_transforms['val'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASLClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ASLClassifierCNN(num_classes=27).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloaders, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "--------------------\n",
      "Processing batch 1/175 for train phase\n",
      "Processing batch 2/175 for train phase\n",
      "Processing batch 3/175 for train phase\n",
      "Processing batch 4/175 for train phase\n",
      "Processing batch 5/175 for train phase\n",
      "Processing batch 6/175 for train phase\n",
      "Processing batch 7/175 for train phase\n",
      "Processing batch 8/175 for train phase\n",
      "Processing batch 9/175 for train phase\n",
      "Processing batch 10/175 for train phase\n",
      "Processing batch 11/175 for train phase\n",
      "Processing batch 12/175 for train phase\n",
      "Processing batch 13/175 for train phase\n",
      "Processing batch 14/175 for train phase\n",
      "Processing batch 15/175 for train phase\n",
      "Processing batch 16/175 for train phase\n",
      "Processing batch 17/175 for train phase\n",
      "Processing batch 18/175 for train phase\n",
      "Processing batch 19/175 for train phase\n",
      "Processing batch 20/175 for train phase\n",
      "Processing batch 21/175 for train phase\n",
      "Processing batch 22/175 for train phase\n",
      "Processing batch 23/175 for train phase\n",
      "Processing batch 24/175 for train phase\n",
      "Processing batch 25/175 for train phase\n",
      "Processing batch 26/175 for train phase\n",
      "Processing batch 27/175 for train phase\n",
      "Processing batch 28/175 for train phase\n",
      "Processing batch 29/175 for train phase\n",
      "Processing batch 30/175 for train phase\n",
      "Processing batch 31/175 for train phase\n",
      "Processing batch 32/175 for train phase\n",
      "Processing batch 33/175 for train phase\n",
      "Processing batch 34/175 for train phase\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 512, 513] at entry 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Example usage of the function\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model_with_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mtrain_model_with_hyperparameters\u001b[0;34m(num_classes, learning_rate, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Iterate over data\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mphase\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m phase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 512, 513] at entry 3\n"
     ]
    }
   ],
   "source": [
    "def train_model_with_hyperparameters(num_classes, learning_rate, batch_size, num_epochs):\n",
    "    # Instantiate model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ASLClassifierCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Dataloaders with specified batch size\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "        'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    }\n",
    "\n",
    "    # Define Loss Function and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                print(f\"Processing batch {i+1}/{len(dataloaders[phase])} for {phase} phase\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Example usage of the function\n",
    "train_model_with_hyperparameters(num_classes=27, learning_rate=0.001, batch_size=32, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'num_classes': 27, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Epoch 1/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 3.2970, Accuracy: 0.0312\n",
      "Train Batch 2/175 - Loss: 3.6344, Accuracy: 0.0625\n",
      "Train Batch 3/175 - Loss: 3.8976, Accuracy: 0.0938\n",
      "Train Batch 4/175 - Loss: 3.4774, Accuracy: 0.0000\n",
      "Train Batch 5/175 - Loss: 3.4682, Accuracy: 0.0000\n",
      "Train Batch 6/175 - Loss: 3.3400, Accuracy: 0.0312\n",
      "Train Batch 7/175 - Loss: 3.3189, Accuracy: 0.0000\n",
      "Train Batch 8/175 - Loss: 3.2758, Accuracy: 0.1250\n",
      "Train Batch 9/175 - Loss: 3.2829, Accuracy: 0.0000\n",
      "Train Batch 10/175 - Loss: 3.2821, Accuracy: 0.0312\n",
      "Train Batch 11/175 - Loss: 3.2824, Accuracy: 0.0000\n",
      "Train Batch 12/175 - Loss: 3.3182, Accuracy: 0.0312\n",
      "Train Batch 13/175 - Loss: 3.2806, Accuracy: 0.0312\n",
      "Train Batch 14/175 - Loss: 3.2660, Accuracy: 0.0938\n",
      "Train Batch 15/175 - Loss: 3.2912, Accuracy: 0.0000\n",
      "Train Batch 16/175 - Loss: 3.3279, Accuracy: 0.0000\n",
      "Train Batch 17/175 - Loss: 3.3192, Accuracy: 0.0000\n",
      "Train Batch 18/175 - Loss: 3.3168, Accuracy: 0.0312\n",
      "Train Batch 19/175 - Loss: 3.2952, Accuracy: 0.0000\n",
      "Train Batch 20/175 - Loss: 3.3354, Accuracy: 0.0000\n",
      "Train Batch 21/175 - Loss: 3.2754, Accuracy: 0.0938\n",
      "Train Batch 22/175 - Loss: 3.2966, Accuracy: 0.0938\n",
      "Train Batch 23/175 - Loss: 3.2957, Accuracy: 0.0625\n",
      "Train Batch 24/175 - Loss: 3.2929, Accuracy: 0.0938\n",
      "Train Batch 25/175 - Loss: 3.2916, Accuracy: 0.0625\n",
      "Train Batch 26/175 - Loss: 3.3000, Accuracy: 0.0000\n",
      "Train Batch 27/175 - Loss: 3.2513, Accuracy: 0.1250\n",
      "Train Batch 28/175 - Loss: 3.2913, Accuracy: 0.0000\n",
      "Train Batch 29/175 - Loss: 3.3152, Accuracy: 0.0625\n",
      "Train Batch 30/175 - Loss: 3.3058, Accuracy: 0.0312\n",
      "Train Batch 31/175 - Loss: 3.2384, Accuracy: 0.0625\n",
      "Train Batch 32/175 - Loss: 3.2825, Accuracy: 0.0625\n",
      "Train Batch 33/175 - Loss: 3.2519, Accuracy: 0.0938\n",
      "Train Batch 34/175 - Loss: 3.2756, Accuracy: 0.0312\n",
      "Train Batch 35/175 - Loss: 3.3429, Accuracy: 0.0000\n",
      "Train Batch 36/175 - Loss: 3.2598, Accuracy: 0.0000\n",
      "Train Batch 37/175 - Loss: 3.2703, Accuracy: 0.0000\n",
      "Train Batch 38/175 - Loss: 3.3171, Accuracy: 0.0312\n",
      "Train Batch 39/175 - Loss: 3.2753, Accuracy: 0.0312\n",
      "Train Batch 40/175 - Loss: 3.2596, Accuracy: 0.0312\n",
      "Train Batch 41/175 - Loss: 3.3049, Accuracy: 0.0938\n",
      "Train Batch 42/175 - Loss: 3.2866, Accuracy: 0.0625\n",
      "Train Batch 43/175 - Loss: 3.2953, Accuracy: 0.0000\n",
      "Train Batch 44/175 - Loss: 3.2947, Accuracy: 0.0625\n",
      "Train Batch 45/175 - Loss: 3.2805, Accuracy: 0.0625\n",
      "Train Batch 46/175 - Loss: 3.2716, Accuracy: 0.0312\n",
      "Train Batch 47/175 - Loss: 3.2665, Accuracy: 0.0625\n",
      "Train Batch 48/175 - Loss: 3.2879, Accuracy: 0.0312\n",
      "Train Batch 49/175 - Loss: 3.2838, Accuracy: 0.0625\n",
      "Train Batch 50/175 - Loss: 3.2793, Accuracy: 0.0312\n",
      "Train Batch 51/175 - Loss: 3.2787, Accuracy: 0.0312\n",
      "Train Batch 52/175 - Loss: 3.2465, Accuracy: 0.0312\n",
      "Train Batch 53/175 - Loss: 3.3027, Accuracy: 0.0000\n",
      "Train Batch 54/175 - Loss: 3.2830, Accuracy: 0.0312\n",
      "Train Batch 55/175 - Loss: 3.2837, Accuracy: 0.0625\n",
      "Train Batch 56/175 - Loss: 3.2938, Accuracy: 0.0000\n",
      "Train Batch 57/175 - Loss: 3.2705, Accuracy: 0.0625\n",
      "Train Batch 58/175 - Loss: 3.2334, Accuracy: 0.0625\n",
      "Train Batch 59/175 - Loss: 3.2711, Accuracy: 0.0312\n",
      "Train Batch 60/175 - Loss: 3.2860, Accuracy: 0.0312\n",
      "Train Batch 61/175 - Loss: 3.2855, Accuracy: 0.0000\n",
      "Train Batch 62/175 - Loss: 3.2560, Accuracy: 0.0312\n",
      "Train Batch 63/175 - Loss: 3.3293, Accuracy: 0.0312\n",
      "Train Batch 64/175 - Loss: 3.2615, Accuracy: 0.0312\n",
      "Train Batch 65/175 - Loss: 3.3153, Accuracy: 0.0938\n",
      "Train Batch 66/175 - Loss: 3.2679, Accuracy: 0.0938\n",
      "Train Batch 67/175 - Loss: 3.2670, Accuracy: 0.0938\n",
      "Train Batch 68/175 - Loss: 3.2457, Accuracy: 0.0312\n",
      "Train Batch 69/175 - Loss: 3.2843, Accuracy: 0.0625\n",
      "Train Batch 70/175 - Loss: 3.3211, Accuracy: 0.0938\n",
      "Train Batch 71/175 - Loss: 3.2348, Accuracy: 0.0000\n",
      "Train Batch 72/175 - Loss: 3.2582, Accuracy: 0.0625\n",
      "Train Batch 73/175 - Loss: 3.2478, Accuracy: 0.0938\n",
      "Train Batch 74/175 - Loss: 3.2429, Accuracy: 0.0625\n",
      "Train Batch 75/175 - Loss: 3.2976, Accuracy: 0.0312\n",
      "Train Batch 76/175 - Loss: 3.2333, Accuracy: 0.0000\n",
      "Train Batch 77/175 - Loss: 3.2599, Accuracy: 0.0000\n",
      "Train Batch 78/175 - Loss: 3.1944, Accuracy: 0.0625\n",
      "Train Batch 79/175 - Loss: 3.2506, Accuracy: 0.0312\n",
      "Train Batch 80/175 - Loss: 3.2321, Accuracy: 0.0938\n",
      "Train Batch 81/175 - Loss: 3.2169, Accuracy: 0.1250\n",
      "Train Batch 82/175 - Loss: 3.2565, Accuracy: 0.0625\n",
      "Train Batch 83/175 - Loss: 3.2718, Accuracy: 0.0938\n",
      "Train Batch 84/175 - Loss: 3.2491, Accuracy: 0.0938\n",
      "Train Batch 85/175 - Loss: 3.2678, Accuracy: 0.0000\n",
      "Train Batch 86/175 - Loss: 3.1219, Accuracy: 0.1250\n",
      "Train Batch 87/175 - Loss: 3.2678, Accuracy: 0.0312\n",
      "Train Batch 88/175 - Loss: 3.2090, Accuracy: 0.0625\n",
      "Train Batch 89/175 - Loss: 3.1382, Accuracy: 0.1250\n",
      "Train Batch 90/175 - Loss: 3.2307, Accuracy: 0.0938\n",
      "Train Batch 91/175 - Loss: 3.1710, Accuracy: 0.0000\n",
      "Train Batch 92/175 - Loss: 3.1749, Accuracy: 0.1250\n",
      "Train Batch 93/175 - Loss: 3.1995, Accuracy: 0.0312\n",
      "Train Batch 94/175 - Loss: 3.2651, Accuracy: 0.0938\n",
      "Train Batch 95/175 - Loss: 3.2089, Accuracy: 0.0312\n",
      "Train Batch 96/175 - Loss: 2.9668, Accuracy: 0.1250\n",
      "Train Batch 97/175 - Loss: 3.1808, Accuracy: 0.1250\n",
      "Train Batch 98/175 - Loss: 3.2115, Accuracy: 0.0312\n",
      "Train Batch 99/175 - Loss: 3.2257, Accuracy: 0.0625\n",
      "Train Batch 100/175 - Loss: 2.9073, Accuracy: 0.2812\n",
      "Train Batch 101/175 - Loss: 3.3863, Accuracy: 0.0938\n",
      "Train Batch 102/175 - Loss: 3.1728, Accuracy: 0.1562\n",
      "Train Batch 103/175 - Loss: 3.0419, Accuracy: 0.1875\n",
      "Train Batch 104/175 - Loss: 3.1236, Accuracy: 0.0938\n",
      "Train Batch 105/175 - Loss: 3.0069, Accuracy: 0.1875\n",
      "Train Batch 106/175 - Loss: 3.0924, Accuracy: 0.2188\n",
      "Train Batch 107/175 - Loss: 3.0922, Accuracy: 0.1250\n",
      "Train Batch 108/175 - Loss: 3.1164, Accuracy: 0.1250\n",
      "Train Batch 109/175 - Loss: 2.9825, Accuracy: 0.1875\n",
      "Train Batch 110/175 - Loss: 3.1826, Accuracy: 0.1250\n",
      "Train Batch 111/175 - Loss: 2.9858, Accuracy: 0.2500\n",
      "Train Batch 112/175 - Loss: 3.1825, Accuracy: 0.2500\n",
      "Train Batch 113/175 - Loss: 2.9551, Accuracy: 0.1875\n",
      "Train Batch 114/175 - Loss: 2.9927, Accuracy: 0.1250\n",
      "Train Batch 115/175 - Loss: 2.7415, Accuracy: 0.2188\n",
      "Train Batch 116/175 - Loss: 2.8352, Accuracy: 0.2188\n",
      "Train Batch 117/175 - Loss: 2.7075, Accuracy: 0.2812\n",
      "Train Batch 118/175 - Loss: 3.0796, Accuracy: 0.0625\n",
      "Train Batch 119/175 - Loss: 2.9101, Accuracy: 0.2500\n",
      "Train Batch 120/175 - Loss: 2.7848, Accuracy: 0.1562\n",
      "Train Batch 121/175 - Loss: 2.6200, Accuracy: 0.1875\n",
      "Train Batch 122/175 - Loss: 2.9512, Accuracy: 0.1562\n",
      "Train Batch 123/175 - Loss: 2.7990, Accuracy: 0.1562\n",
      "Train Batch 124/175 - Loss: 2.9544, Accuracy: 0.2188\n",
      "Train Batch 125/175 - Loss: 2.4684, Accuracy: 0.2500\n",
      "Train Batch 126/175 - Loss: 3.3423, Accuracy: 0.1250\n",
      "Train Batch 127/175 - Loss: 3.0674, Accuracy: 0.1562\n",
      "Train Batch 128/175 - Loss: 2.9533, Accuracy: 0.1250\n",
      "Train Batch 129/175 - Loss: 3.2095, Accuracy: 0.2500\n",
      "Train Batch 130/175 - Loss: 2.9532, Accuracy: 0.3750\n",
      "Train Batch 131/175 - Loss: 2.6843, Accuracy: 0.2812\n",
      "Train Batch 132/175 - Loss: 2.9157, Accuracy: 0.3125\n",
      "Train Batch 133/175 - Loss: 2.8478, Accuracy: 0.2812\n",
      "Train Batch 134/175 - Loss: 2.9736, Accuracy: 0.2188\n",
      "Train Batch 135/175 - Loss: 2.9790, Accuracy: 0.2812\n",
      "Train Batch 136/175 - Loss: 2.9463, Accuracy: 0.2500\n",
      "Train Batch 137/175 - Loss: 2.7701, Accuracy: 0.3438\n",
      "Train Batch 138/175 - Loss: 2.8567, Accuracy: 0.2188\n",
      "Train Batch 139/175 - Loss: 2.8150, Accuracy: 0.2188\n",
      "Train Batch 140/175 - Loss: 2.6641, Accuracy: 0.2812\n",
      "Train Batch 141/175 - Loss: 2.7501, Accuracy: 0.1250\n",
      "Train Batch 142/175 - Loss: 2.5781, Accuracy: 0.2812\n",
      "Train Batch 143/175 - Loss: 2.8217, Accuracy: 0.2188\n",
      "Train Batch 144/175 - Loss: 2.8764, Accuracy: 0.1562\n",
      "Train Batch 145/175 - Loss: 2.7924, Accuracy: 0.2500\n",
      "Train Batch 146/175 - Loss: 2.5542, Accuracy: 0.3438\n",
      "Train Batch 147/175 - Loss: 2.8058, Accuracy: 0.3125\n",
      "Train Batch 148/175 - Loss: 2.4506, Accuracy: 0.4062\n",
      "Train Batch 149/175 - Loss: 2.8573, Accuracy: 0.2812\n",
      "Train Batch 150/175 - Loss: 2.7670, Accuracy: 0.2188\n",
      "Train Batch 151/175 - Loss: 2.7034, Accuracy: 0.1250\n",
      "Train Batch 152/175 - Loss: 2.8273, Accuracy: 0.2812\n",
      "Train Batch 153/175 - Loss: 2.4404, Accuracy: 0.2812\n",
      "Train Batch 154/175 - Loss: 2.5413, Accuracy: 0.3125\n",
      "Train Batch 155/175 - Loss: 2.2382, Accuracy: 0.4688\n",
      "Train Batch 156/175 - Loss: 2.5867, Accuracy: 0.1562\n",
      "Train Batch 157/175 - Loss: 2.3887, Accuracy: 0.3125\n",
      "Train Batch 158/175 - Loss: 2.7615, Accuracy: 0.3750\n",
      "Train Batch 159/175 - Loss: 3.2945, Accuracy: 0.1562\n",
      "Train Batch 160/175 - Loss: 2.4875, Accuracy: 0.3125\n",
      "Train Batch 161/175 - Loss: 2.7800, Accuracy: 0.1562\n",
      "Train Batch 162/175 - Loss: 2.6810, Accuracy: 0.1875\n",
      "Train Batch 163/175 - Loss: 2.9594, Accuracy: 0.1875\n",
      "Train Batch 164/175 - Loss: 2.5457, Accuracy: 0.3125\n",
      "Train Batch 165/175 - Loss: 2.7074, Accuracy: 0.2812\n",
      "Train Batch 166/175 - Loss: 2.5419, Accuracy: 0.3438\n",
      "Train Batch 167/175 - Loss: 2.6102, Accuracy: 0.1250\n",
      "Train Batch 168/175 - Loss: 2.5378, Accuracy: 0.4062\n",
      "Train Batch 169/175 - Loss: 2.5216, Accuracy: 0.3438\n",
      "Train Batch 170/175 - Loss: 2.2767, Accuracy: 0.3750\n",
      "Train Batch 171/175 - Loss: 2.2872, Accuracy: 0.3750\n",
      "Train Batch 172/175 - Loss: 2.5480, Accuracy: 0.1875\n",
      "Train Batch 173/175 - Loss: 2.7443, Accuracy: 0.1875\n",
      "Train Batch 174/175 - Loss: 2.4084, Accuracy: 0.3750\n",
      "Train Batch 175/175 - Loss: 2.3657, Accuracy: 0.1579\n",
      "Train Epoch Loss: 3.0748 Acc: 0.1294\n",
      "Val Batch 1/44 - Loss: 2.9346, Accuracy: 0.0938\n",
      "Val Batch 2/44 - Loss: 2.9819, Accuracy: 0.2188\n",
      "Val Batch 3/44 - Loss: 2.1272, Accuracy: 0.5000\n",
      "Val Batch 4/44 - Loss: 2.1267, Accuracy: 0.4688\n",
      "Val Batch 5/44 - Loss: 2.0362, Accuracy: 0.5000\n",
      "Val Batch 6/44 - Loss: 1.7255, Accuracy: 0.6875\n",
      "Val Batch 7/44 - Loss: 2.2667, Accuracy: 0.5000\n",
      "Val Batch 8/44 - Loss: 1.4593, Accuracy: 0.8750\n",
      "Val Batch 9/44 - Loss: 2.3185, Accuracy: 0.3750\n",
      "Val Batch 10/44 - Loss: 2.8760, Accuracy: 0.0938\n",
      "Val Batch 11/44 - Loss: 2.1787, Accuracy: 0.6250\n",
      "Val Batch 12/44 - Loss: 2.1630, Accuracy: 0.5312\n",
      "Val Batch 13/44 - Loss: 1.9367, Accuracy: 0.6562\n",
      "Val Batch 14/44 - Loss: 1.9933, Accuracy: 0.6250\n",
      "Val Batch 15/44 - Loss: 2.0264, Accuracy: 0.4375\n",
      "Val Batch 16/44 - Loss: 2.5795, Accuracy: 0.1250\n",
      "Val Batch 17/44 - Loss: 2.6003, Accuracy: 0.0625\n",
      "Val Batch 18/44 - Loss: 2.2764, Accuracy: 0.4688\n",
      "Val Batch 19/44 - Loss: 2.0249, Accuracy: 0.5625\n",
      "Val Batch 20/44 - Loss: 1.2745, Accuracy: 0.8750\n",
      "Val Batch 21/44 - Loss: 1.7570, Accuracy: 0.6875\n",
      "Val Batch 22/44 - Loss: 2.0241, Accuracy: 0.4062\n",
      "Val Batch 23/44 - Loss: 1.9931, Accuracy: 0.6562\n",
      "Val Batch 24/44 - Loss: 2.1578, Accuracy: 0.5000\n",
      "Val Batch 25/44 - Loss: 2.2752, Accuracy: 0.5312\n",
      "Val Batch 26/44 - Loss: 2.2177, Accuracy: 0.2812\n",
      "Val Batch 27/44 - Loss: 2.4501, Accuracy: 0.2188\n",
      "Val Batch 28/44 - Loss: 2.8530, Accuracy: 0.1250\n",
      "Val Batch 29/44 - Loss: 2.5839, Accuracy: 0.0938\n",
      "Val Batch 30/44 - Loss: 1.9897, Accuracy: 0.6562\n",
      "Val Batch 31/44 - Loss: 1.5000, Accuracy: 0.7500\n",
      "Val Batch 32/44 - Loss: 1.4698, Accuracy: 0.8125\n",
      "Val Batch 33/44 - Loss: 1.2214, Accuracy: 0.9062\n",
      "Val Batch 34/44 - Loss: 1.2546, Accuracy: 0.9062\n",
      "Val Batch 35/44 - Loss: 3.3543, Accuracy: 0.0312\n",
      "Val Batch 36/44 - Loss: 3.2007, Accuracy: 0.0312\n",
      "Val Batch 37/44 - Loss: 2.7139, Accuracy: 0.1250\n",
      "Val Batch 38/44 - Loss: 2.4080, Accuracy: 0.1250\n",
      "Val Batch 39/44 - Loss: 2.4280, Accuracy: 0.1250\n",
      "Val Batch 40/44 - Loss: 2.4089, Accuracy: 0.4688\n",
      "Val Batch 41/44 - Loss: 2.4014, Accuracy: 0.4375\n",
      "Val Batch 42/44 - Loss: 1.9228, Accuracy: 0.6250\n",
      "Val Batch 43/44 - Loss: 2.1216, Accuracy: 0.3750\n",
      "Val Batch 44/44 - Loss: 1.9645, Accuracy: 0.3077\n",
      "Val Epoch Loss: 2.1959 Acc: 0.4429\n",
      "Epoch 2/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 2.2182, Accuracy: 0.4688\n",
      "Train Batch 2/175 - Loss: 1.8227, Accuracy: 0.4375\n",
      "Train Batch 3/175 - Loss: 2.2681, Accuracy: 0.3438\n",
      "Train Batch 4/175 - Loss: 2.0221, Accuracy: 0.5000\n",
      "Train Batch 5/175 - Loss: 2.3547, Accuracy: 0.3438\n",
      "Train Batch 6/175 - Loss: 2.0322, Accuracy: 0.3750\n",
      "Train Batch 7/175 - Loss: 2.1186, Accuracy: 0.4375\n",
      "Train Batch 8/175 - Loss: 2.9499, Accuracy: 0.2500\n",
      "Train Batch 9/175 - Loss: 2.6017, Accuracy: 0.4688\n",
      "Train Batch 10/175 - Loss: 2.1658, Accuracy: 0.4062\n",
      "Train Batch 11/175 - Loss: 2.1216, Accuracy: 0.4375\n",
      "Train Batch 12/175 - Loss: 2.4233, Accuracy: 0.3750\n",
      "Train Batch 13/175 - Loss: 1.7149, Accuracy: 0.6875\n",
      "Train Batch 14/175 - Loss: 2.3421, Accuracy: 0.5000\n",
      "Train Batch 15/175 - Loss: 1.9689, Accuracy: 0.5625\n",
      "Train Batch 16/175 - Loss: 2.3222, Accuracy: 0.2812\n",
      "Train Batch 17/175 - Loss: 2.4242, Accuracy: 0.2812\n",
      "Train Batch 18/175 - Loss: 1.8787, Accuracy: 0.4375\n",
      "Train Batch 19/175 - Loss: 2.3564, Accuracy: 0.3750\n",
      "Train Batch 20/175 - Loss: 2.2534, Accuracy: 0.2812\n",
      "Train Batch 21/175 - Loss: 2.3249, Accuracy: 0.3438\n",
      "Train Batch 22/175 - Loss: 1.9679, Accuracy: 0.5312\n",
      "Train Batch 23/175 - Loss: 2.0684, Accuracy: 0.4688\n",
      "Train Batch 24/175 - Loss: 2.0554, Accuracy: 0.3125\n",
      "Train Batch 25/175 - Loss: 2.5830, Accuracy: 0.4375\n",
      "Train Batch 26/175 - Loss: 1.8499, Accuracy: 0.5000\n",
      "Train Batch 27/175 - Loss: 1.9399, Accuracy: 0.5312\n",
      "Train Batch 28/175 - Loss: 2.2634, Accuracy: 0.2812\n",
      "Train Batch 29/175 - Loss: 1.9590, Accuracy: 0.4688\n",
      "Train Batch 30/175 - Loss: 2.0902, Accuracy: 0.4375\n",
      "Train Batch 31/175 - Loss: 1.8582, Accuracy: 0.5312\n",
      "Train Batch 32/175 - Loss: 2.1728, Accuracy: 0.3750\n",
      "Train Batch 33/175 - Loss: 2.0723, Accuracy: 0.3438\n",
      "Train Batch 34/175 - Loss: 1.5975, Accuracy: 0.5938\n",
      "Train Batch 35/175 - Loss: 2.1226, Accuracy: 0.4062\n",
      "Train Batch 36/175 - Loss: 1.9206, Accuracy: 0.4688\n",
      "Train Batch 37/175 - Loss: 1.8477, Accuracy: 0.4375\n",
      "Train Batch 38/175 - Loss: 2.1963, Accuracy: 0.5938\n",
      "Train Batch 39/175 - Loss: 1.5157, Accuracy: 0.5625\n",
      "Train Batch 40/175 - Loss: 2.3050, Accuracy: 0.5000\n",
      "Train Batch 41/175 - Loss: 1.8702, Accuracy: 0.4688\n",
      "Train Batch 42/175 - Loss: 1.7923, Accuracy: 0.5312\n",
      "Train Batch 43/175 - Loss: 1.9130, Accuracy: 0.4375\n",
      "Train Batch 44/175 - Loss: 1.9034, Accuracy: 0.5000\n",
      "Train Batch 45/175 - Loss: 1.9829, Accuracy: 0.4688\n",
      "Train Batch 46/175 - Loss: 1.5826, Accuracy: 0.5312\n",
      "Train Batch 47/175 - Loss: 1.8573, Accuracy: 0.5000\n",
      "Train Batch 48/175 - Loss: 1.8394, Accuracy: 0.5312\n",
      "Train Batch 49/175 - Loss: 1.6652, Accuracy: 0.5000\n",
      "Train Batch 50/175 - Loss: 1.6645, Accuracy: 0.5938\n",
      "Train Batch 51/175 - Loss: 1.8686, Accuracy: 0.4688\n",
      "Train Batch 52/175 - Loss: 1.4731, Accuracy: 0.6562\n",
      "Train Batch 53/175 - Loss: 1.4067, Accuracy: 0.5625\n",
      "Train Batch 54/175 - Loss: 1.6305, Accuracy: 0.4375\n",
      "Train Batch 55/175 - Loss: 1.3099, Accuracy: 0.6562\n",
      "Train Batch 56/175 - Loss: 1.7824, Accuracy: 0.5312\n",
      "Train Batch 57/175 - Loss: 1.6724, Accuracy: 0.5312\n",
      "Train Batch 58/175 - Loss: 1.3776, Accuracy: 0.6250\n",
      "Train Batch 59/175 - Loss: 2.0247, Accuracy: 0.4688\n",
      "Train Batch 60/175 - Loss: 1.9883, Accuracy: 0.5625\n",
      "Train Batch 61/175 - Loss: 1.4678, Accuracy: 0.6250\n",
      "Train Batch 62/175 - Loss: 1.5635, Accuracy: 0.5312\n",
      "Train Batch 63/175 - Loss: 1.6253, Accuracy: 0.5000\n",
      "Train Batch 64/175 - Loss: 1.1413, Accuracy: 0.7188\n",
      "Train Batch 65/175 - Loss: 1.6292, Accuracy: 0.5312\n",
      "Train Batch 66/175 - Loss: 1.9724, Accuracy: 0.5312\n",
      "Train Batch 67/175 - Loss: 1.7955, Accuracy: 0.5000\n",
      "Train Batch 68/175 - Loss: 1.3379, Accuracy: 0.6875\n",
      "Train Batch 69/175 - Loss: 1.6866, Accuracy: 0.5312\n",
      "Train Batch 70/175 - Loss: 1.5092, Accuracy: 0.5000\n",
      "Train Batch 71/175 - Loss: 1.4460, Accuracy: 0.5938\n",
      "Train Batch 72/175 - Loss: 1.4492, Accuracy: 0.5000\n",
      "Train Batch 73/175 - Loss: 1.3872, Accuracy: 0.6562\n",
      "Train Batch 74/175 - Loss: 1.5406, Accuracy: 0.5938\n",
      "Train Batch 75/175 - Loss: 1.3405, Accuracy: 0.6562\n",
      "Train Batch 76/175 - Loss: 1.6368, Accuracy: 0.5938\n",
      "Train Batch 77/175 - Loss: 1.5418, Accuracy: 0.5938\n",
      "Train Batch 78/175 - Loss: 1.0815, Accuracy: 0.6562\n",
      "Train Batch 79/175 - Loss: 1.5566, Accuracy: 0.5625\n",
      "Train Batch 80/175 - Loss: 1.2592, Accuracy: 0.6562\n",
      "Train Batch 81/175 - Loss: 1.5745, Accuracy: 0.5625\n",
      "Train Batch 82/175 - Loss: 1.3960, Accuracy: 0.6562\n",
      "Train Batch 83/175 - Loss: 1.9503, Accuracy: 0.5625\n",
      "Train Batch 84/175 - Loss: 1.4470, Accuracy: 0.5000\n",
      "Train Batch 85/175 - Loss: 1.4281, Accuracy: 0.6562\n",
      "Train Batch 86/175 - Loss: 1.1933, Accuracy: 0.6875\n",
      "Train Batch 87/175 - Loss: 1.4363, Accuracy: 0.6875\n",
      "Train Batch 88/175 - Loss: 1.9434, Accuracy: 0.5312\n",
      "Train Batch 89/175 - Loss: 1.4366, Accuracy: 0.6875\n",
      "Train Batch 90/175 - Loss: 1.7152, Accuracy: 0.6250\n",
      "Train Batch 91/175 - Loss: 1.5251, Accuracy: 0.5938\n",
      "Train Batch 92/175 - Loss: 1.0065, Accuracy: 0.7500\n",
      "Train Batch 93/175 - Loss: 1.6668, Accuracy: 0.5625\n",
      "Train Batch 94/175 - Loss: 1.3390, Accuracy: 0.5938\n",
      "Train Batch 95/175 - Loss: 1.4250, Accuracy: 0.6875\n",
      "Train Batch 96/175 - Loss: 1.4797, Accuracy: 0.5625\n",
      "Train Batch 97/175 - Loss: 1.3884, Accuracy: 0.6250\n",
      "Train Batch 98/175 - Loss: 1.3022, Accuracy: 0.6875\n",
      "Train Batch 99/175 - Loss: 1.1798, Accuracy: 0.6562\n",
      "Train Batch 100/175 - Loss: 0.8843, Accuracy: 0.7812\n",
      "Train Batch 101/175 - Loss: 1.5284, Accuracy: 0.5625\n",
      "Train Batch 102/175 - Loss: 1.4333, Accuracy: 0.5938\n",
      "Train Batch 103/175 - Loss: 1.1884, Accuracy: 0.5625\n",
      "Train Batch 104/175 - Loss: 1.4174, Accuracy: 0.5625\n",
      "Train Batch 105/175 - Loss: 1.5946, Accuracy: 0.5625\n",
      "Train Batch 106/175 - Loss: 1.1909, Accuracy: 0.6562\n",
      "Train Batch 107/175 - Loss: 0.9499, Accuracy: 0.7812\n",
      "Train Batch 108/175 - Loss: 1.2196, Accuracy: 0.5625\n",
      "Train Batch 109/175 - Loss: 1.7238, Accuracy: 0.5625\n",
      "Train Batch 110/175 - Loss: 1.0146, Accuracy: 0.7500\n",
      "Train Batch 111/175 - Loss: 1.3268, Accuracy: 0.6562\n",
      "Train Batch 112/175 - Loss: 1.1812, Accuracy: 0.6250\n",
      "Train Batch 113/175 - Loss: 1.5919, Accuracy: 0.5000\n",
      "Train Batch 114/175 - Loss: 1.0592, Accuracy: 0.6250\n",
      "Train Batch 115/175 - Loss: 1.0672, Accuracy: 0.6875\n",
      "Train Batch 116/175 - Loss: 1.7302, Accuracy: 0.5625\n",
      "Train Batch 117/175 - Loss: 1.1360, Accuracy: 0.6562\n",
      "Train Batch 118/175 - Loss: 1.2486, Accuracy: 0.6562\n",
      "Train Batch 119/175 - Loss: 1.7166, Accuracy: 0.5000\n",
      "Train Batch 120/175 - Loss: 1.7908, Accuracy: 0.5312\n",
      "Train Batch 121/175 - Loss: 1.4845, Accuracy: 0.5625\n",
      "Train Batch 122/175 - Loss: 0.9426, Accuracy: 0.7812\n",
      "Train Batch 123/175 - Loss: 0.8300, Accuracy: 0.7812\n",
      "Train Batch 124/175 - Loss: 1.1784, Accuracy: 0.6875\n",
      "Train Batch 125/175 - Loss: 0.9882, Accuracy: 0.6875\n",
      "Train Batch 126/175 - Loss: 0.8452, Accuracy: 0.6562\n",
      "Train Batch 127/175 - Loss: 1.3170, Accuracy: 0.6562\n",
      "Train Batch 128/175 - Loss: 2.1016, Accuracy: 0.4062\n",
      "Train Batch 129/175 - Loss: 0.9170, Accuracy: 0.7188\n",
      "Train Batch 130/175 - Loss: 1.5112, Accuracy: 0.6875\n",
      "Train Batch 131/175 - Loss: 0.9765, Accuracy: 0.6875\n",
      "Train Batch 132/175 - Loss: 0.9366, Accuracy: 0.7500\n",
      "Train Batch 133/175 - Loss: 1.1127, Accuracy: 0.6562\n",
      "Train Batch 134/175 - Loss: 1.2578, Accuracy: 0.6875\n",
      "Train Batch 135/175 - Loss: 1.3596, Accuracy: 0.6562\n",
      "Train Batch 136/175 - Loss: 1.3931, Accuracy: 0.5938\n",
      "Train Batch 137/175 - Loss: 0.8446, Accuracy: 0.9375\n",
      "Train Batch 138/175 - Loss: 0.9354, Accuracy: 0.6875\n",
      "Train Batch 139/175 - Loss: 1.7805, Accuracy: 0.5312\n",
      "Train Batch 140/175 - Loss: 1.6057, Accuracy: 0.6250\n",
      "Train Batch 141/175 - Loss: 0.9369, Accuracy: 0.7188\n",
      "Train Batch 142/175 - Loss: 1.0335, Accuracy: 0.6875\n",
      "Train Batch 143/175 - Loss: 1.3075, Accuracy: 0.6250\n",
      "Train Batch 144/175 - Loss: 1.2992, Accuracy: 0.6562\n",
      "Train Batch 145/175 - Loss: 1.1868, Accuracy: 0.8125\n",
      "Train Batch 146/175 - Loss: 1.2823, Accuracy: 0.7188\n",
      "Train Batch 147/175 - Loss: 1.3082, Accuracy: 0.6562\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def train_and_plot_hyperparameter_effects(hyperparameters, image_datasets, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the model with different hyperparameter combinations and plots results.\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (list of dict): List of hyperparameter dictionaries.\n",
    "        image_datasets (dict): Datasets for 'train' and 'val' datasets.\n",
    "        num_epochs (int): Number of epochs for each training run.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with hyperparameter configurations and corresponding results.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = []\n",
    "\n",
    "    for config in hyperparameters:\n",
    "        print(f\"Training with hyperparameters: {config}\")\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        num_classes = config.get('num_classes', 27)\n",
    "        learning_rate = config.get('learning_rate', 0.001)\n",
    "        batch_size = config.get('batch_size', 32)\n",
    "\n",
    "        # Initialize model, criterion, optimizer\n",
    "        model = ASLClassifierCNN(num_classes=num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Update dataloaders for the current batch size\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "            'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        }\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print('-' * 20)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                total_samples = 0\n",
    "\n",
    "                for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    total_samples += labels.size(0)\n",
    "\n",
    "                    batch_loss = loss.item()\n",
    "                    batch_acc = torch.sum(preds == labels.data).double() / labels.size(0)\n",
    "\n",
    "                    print(f\"{phase.capitalize()} Batch {i+1}/{len(dataloaders[phase])} - Loss: {batch_loss:.4f}, Accuracy: {batch_acc:.4f}\")\n",
    "\n",
    "                epoch_loss = running_loss / len(image_datasets[phase])\n",
    "                epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_accs.append(epoch_acc.item())\n",
    "                else:\n",
    "                    val_losses.append(epoch_loss)\n",
    "                    val_accs.append(epoch_acc.item())\n",
    "\n",
    "                print(f\"{phase.capitalize()} Epoch Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        # Save results for this configuration\n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'train_loss': train_losses[-1],\n",
    "            'val_loss': val_losses[-1],\n",
    "            'train_acc': train_accs[-1],\n",
    "            'val_acc': val_accs[-1]\n",
    "        })\n",
    "\n",
    "        # Plot learning curves for this configuration\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss Curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f\"Loss Curve ({config})\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy Curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, num_epochs + 1), train_accs, label='Train Accuracy')\n",
    "        plt.plot(range(1, num_epochs + 1), val_accs, label='Val Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f\"Accuracy Curve ({config})\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Convert results to a DataFrame for further analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"All configurations completed!\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example of hyperparameter configurations to test\n",
    "hyperparameter_configs = [\n",
    "    {'num_classes': 27, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "    {'num_classes': 27, 'learning_rate': 0.0005, 'batch_size': 64},\n",
    "    {'num_classes': 27, 'learning_rate': 0.005, 'batch_size': 16},\n",
    "]\n",
    "\n",
    "# Run training and plot results\n",
    "results_df = train_and_plot_hyperparameter_effects(hyperparameter_configs, image_datasets, num_epochs=10)\n",
    "\n",
    "# Display summary results\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
