{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducability\n",
    "torch.manual_seed(311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(64),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.Resize(64),\n",
    "        #transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder('Final_Data_Reduced/Train', data_transforms['train']),\n",
    "    'val': datasets.ImageFolder('Final_Data_Reduced/Validation', data_transforms['val'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASLClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ASLClassifierCNN(num_classes=27).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloaders, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "--------------------\n",
      "Processing batch 1/175 for train phase\n",
      "Processing batch 2/175 for train phase\n",
      "Processing batch 3/175 for train phase\n",
      "Processing batch 4/175 for train phase\n",
      "Processing batch 5/175 for train phase\n",
      "Processing batch 6/175 for train phase\n",
      "Processing batch 7/175 for train phase\n",
      "Processing batch 8/175 for train phase\n",
      "Processing batch 9/175 for train phase\n",
      "Processing batch 10/175 for train phase\n",
      "Processing batch 11/175 for train phase\n",
      "Processing batch 12/175 for train phase\n",
      "Processing batch 13/175 for train phase\n",
      "Processing batch 14/175 for train phase\n",
      "Processing batch 15/175 for train phase\n",
      "Processing batch 16/175 for train phase\n",
      "Processing batch 17/175 for train phase\n",
      "Processing batch 18/175 for train phase\n",
      "Processing batch 19/175 for train phase\n",
      "Processing batch 20/175 for train phase\n",
      "Processing batch 21/175 for train phase\n",
      "Processing batch 22/175 for train phase\n",
      "Processing batch 23/175 for train phase\n",
      "Processing batch 24/175 for train phase\n",
      "Processing batch 25/175 for train phase\n",
      "Processing batch 26/175 for train phase\n",
      "Processing batch 27/175 for train phase\n",
      "Processing batch 28/175 for train phase\n",
      "Processing batch 29/175 for train phase\n",
      "Processing batch 30/175 for train phase\n",
      "Processing batch 31/175 for train phase\n",
      "Processing batch 32/175 for train phase\n",
      "Processing batch 33/175 for train phase\n",
      "Processing batch 34/175 for train phase\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 512, 513] at entry 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Example usage of the function\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model_with_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mtrain_model_with_hyperparameters\u001b[0;34m(num_classes, learning_rate, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Iterate over data\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mphase\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m phase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 512, 513] at entry 3\n"
     ]
    }
   ],
   "source": [
    "def train_model_with_hyperparameters(num_classes, learning_rate, batch_size, num_epochs):\n",
    "    # Instantiate model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ASLClassifierCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Dataloaders with specified batch size\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "        'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    }\n",
    "\n",
    "    # Define Loss Function and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                print(f\"Processing batch {i+1}/{len(dataloaders[phase])} for {phase} phase\")\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Example usage of the function\n",
    "train_model_with_hyperparameters(num_classes=27, learning_rate=0.001, batch_size=32, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'num_classes': 27, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Epoch 1/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 3.2970, Accuracy: 0.0312\n",
      "Train Batch 2/175 - Loss: 3.6344, Accuracy: 0.0625\n",
      "Train Batch 3/175 - Loss: 3.8976, Accuracy: 0.0938\n",
      "Train Batch 4/175 - Loss: 3.4774, Accuracy: 0.0000\n",
      "Train Batch 5/175 - Loss: 3.4682, Accuracy: 0.0000\n",
      "Train Batch 6/175 - Loss: 3.3400, Accuracy: 0.0312\n",
      "Train Batch 7/175 - Loss: 3.3189, Accuracy: 0.0000\n",
      "Train Batch 8/175 - Loss: 3.2758, Accuracy: 0.1250\n",
      "Train Batch 9/175 - Loss: 3.2829, Accuracy: 0.0000\n",
      "Train Batch 10/175 - Loss: 3.2821, Accuracy: 0.0312\n",
      "Train Batch 11/175 - Loss: 3.2824, Accuracy: 0.0000\n",
      "Train Batch 12/175 - Loss: 3.3182, Accuracy: 0.0312\n",
      "Train Batch 13/175 - Loss: 3.2806, Accuracy: 0.0312\n",
      "Train Batch 14/175 - Loss: 3.2660, Accuracy: 0.0938\n",
      "Train Batch 15/175 - Loss: 3.2912, Accuracy: 0.0000\n",
      "Train Batch 16/175 - Loss: 3.3279, Accuracy: 0.0000\n",
      "Train Batch 17/175 - Loss: 3.3192, Accuracy: 0.0000\n",
      "Train Batch 18/175 - Loss: 3.3168, Accuracy: 0.0312\n",
      "Train Batch 19/175 - Loss: 3.2952, Accuracy: 0.0000\n",
      "Train Batch 20/175 - Loss: 3.3354, Accuracy: 0.0000\n",
      "Train Batch 21/175 - Loss: 3.2754, Accuracy: 0.0938\n",
      "Train Batch 22/175 - Loss: 3.2966, Accuracy: 0.0938\n",
      "Train Batch 23/175 - Loss: 3.2957, Accuracy: 0.0625\n",
      "Train Batch 24/175 - Loss: 3.2929, Accuracy: 0.0938\n",
      "Train Batch 25/175 - Loss: 3.2916, Accuracy: 0.0625\n",
      "Train Batch 26/175 - Loss: 3.3000, Accuracy: 0.0000\n",
      "Train Batch 27/175 - Loss: 3.2513, Accuracy: 0.1250\n",
      "Train Batch 28/175 - Loss: 3.2913, Accuracy: 0.0000\n",
      "Train Batch 29/175 - Loss: 3.3152, Accuracy: 0.0625\n",
      "Train Batch 30/175 - Loss: 3.3058, Accuracy: 0.0312\n",
      "Train Batch 31/175 - Loss: 3.2384, Accuracy: 0.0625\n",
      "Train Batch 32/175 - Loss: 3.2825, Accuracy: 0.0625\n",
      "Train Batch 33/175 - Loss: 3.2519, Accuracy: 0.0938\n",
      "Train Batch 34/175 - Loss: 3.2756, Accuracy: 0.0312\n",
      "Train Batch 35/175 - Loss: 3.3429, Accuracy: 0.0000\n",
      "Train Batch 36/175 - Loss: 3.2598, Accuracy: 0.0000\n",
      "Train Batch 37/175 - Loss: 3.2703, Accuracy: 0.0000\n",
      "Train Batch 38/175 - Loss: 3.3171, Accuracy: 0.0312\n",
      "Train Batch 39/175 - Loss: 3.2753, Accuracy: 0.0312\n",
      "Train Batch 40/175 - Loss: 3.2596, Accuracy: 0.0312\n",
      "Train Batch 41/175 - Loss: 3.3049, Accuracy: 0.0938\n",
      "Train Batch 42/175 - Loss: 3.2866, Accuracy: 0.0625\n",
      "Train Batch 43/175 - Loss: 3.2953, Accuracy: 0.0000\n",
      "Train Batch 44/175 - Loss: 3.2947, Accuracy: 0.0625\n",
      "Train Batch 45/175 - Loss: 3.2805, Accuracy: 0.0625\n",
      "Train Batch 46/175 - Loss: 3.2716, Accuracy: 0.0312\n",
      "Train Batch 47/175 - Loss: 3.2665, Accuracy: 0.0625\n",
      "Train Batch 48/175 - Loss: 3.2879, Accuracy: 0.0312\n",
      "Train Batch 49/175 - Loss: 3.2838, Accuracy: 0.0625\n",
      "Train Batch 50/175 - Loss: 3.2793, Accuracy: 0.0312\n",
      "Train Batch 51/175 - Loss: 3.2787, Accuracy: 0.0312\n",
      "Train Batch 52/175 - Loss: 3.2465, Accuracy: 0.0312\n",
      "Train Batch 53/175 - Loss: 3.3027, Accuracy: 0.0000\n",
      "Train Batch 54/175 - Loss: 3.2830, Accuracy: 0.0312\n",
      "Train Batch 55/175 - Loss: 3.2837, Accuracy: 0.0625\n",
      "Train Batch 56/175 - Loss: 3.2938, Accuracy: 0.0000\n",
      "Train Batch 57/175 - Loss: 3.2705, Accuracy: 0.0625\n",
      "Train Batch 58/175 - Loss: 3.2334, Accuracy: 0.0625\n",
      "Train Batch 59/175 - Loss: 3.2711, Accuracy: 0.0312\n",
      "Train Batch 60/175 - Loss: 3.2860, Accuracy: 0.0312\n",
      "Train Batch 61/175 - Loss: 3.2855, Accuracy: 0.0000\n",
      "Train Batch 62/175 - Loss: 3.2560, Accuracy: 0.0312\n",
      "Train Batch 63/175 - Loss: 3.3293, Accuracy: 0.0312\n",
      "Train Batch 64/175 - Loss: 3.2615, Accuracy: 0.0312\n",
      "Train Batch 65/175 - Loss: 3.3153, Accuracy: 0.0938\n",
      "Train Batch 66/175 - Loss: 3.2679, Accuracy: 0.0938\n",
      "Train Batch 67/175 - Loss: 3.2670, Accuracy: 0.0938\n",
      "Train Batch 68/175 - Loss: 3.2457, Accuracy: 0.0312\n",
      "Train Batch 69/175 - Loss: 3.2843, Accuracy: 0.0625\n",
      "Train Batch 70/175 - Loss: 3.3211, Accuracy: 0.0938\n",
      "Train Batch 71/175 - Loss: 3.2348, Accuracy: 0.0000\n",
      "Train Batch 72/175 - Loss: 3.2582, Accuracy: 0.0625\n",
      "Train Batch 73/175 - Loss: 3.2478, Accuracy: 0.0938\n",
      "Train Batch 74/175 - Loss: 3.2429, Accuracy: 0.0625\n",
      "Train Batch 75/175 - Loss: 3.2976, Accuracy: 0.0312\n",
      "Train Batch 76/175 - Loss: 3.2333, Accuracy: 0.0000\n",
      "Train Batch 77/175 - Loss: 3.2599, Accuracy: 0.0000\n",
      "Train Batch 78/175 - Loss: 3.1944, Accuracy: 0.0625\n",
      "Train Batch 79/175 - Loss: 3.2506, Accuracy: 0.0312\n",
      "Train Batch 80/175 - Loss: 3.2321, Accuracy: 0.0938\n",
      "Train Batch 81/175 - Loss: 3.2169, Accuracy: 0.1250\n",
      "Train Batch 82/175 - Loss: 3.2565, Accuracy: 0.0625\n",
      "Train Batch 83/175 - Loss: 3.2718, Accuracy: 0.0938\n",
      "Train Batch 84/175 - Loss: 3.2491, Accuracy: 0.0938\n",
      "Train Batch 85/175 - Loss: 3.2678, Accuracy: 0.0000\n",
      "Train Batch 86/175 - Loss: 3.1219, Accuracy: 0.1250\n",
      "Train Batch 87/175 - Loss: 3.2678, Accuracy: 0.0312\n",
      "Train Batch 88/175 - Loss: 3.2090, Accuracy: 0.0625\n",
      "Train Batch 89/175 - Loss: 3.1382, Accuracy: 0.1250\n",
      "Train Batch 90/175 - Loss: 3.2307, Accuracy: 0.0938\n",
      "Train Batch 91/175 - Loss: 3.1710, Accuracy: 0.0000\n",
      "Train Batch 92/175 - Loss: 3.1749, Accuracy: 0.1250\n",
      "Train Batch 93/175 - Loss: 3.1995, Accuracy: 0.0312\n",
      "Train Batch 94/175 - Loss: 3.2651, Accuracy: 0.0938\n",
      "Train Batch 95/175 - Loss: 3.2089, Accuracy: 0.0312\n",
      "Train Batch 96/175 - Loss: 2.9668, Accuracy: 0.1250\n",
      "Train Batch 97/175 - Loss: 3.1808, Accuracy: 0.1250\n",
      "Train Batch 98/175 - Loss: 3.2115, Accuracy: 0.0312\n",
      "Train Batch 99/175 - Loss: 3.2257, Accuracy: 0.0625\n",
      "Train Batch 100/175 - Loss: 2.9073, Accuracy: 0.2812\n",
      "Train Batch 101/175 - Loss: 3.3863, Accuracy: 0.0938\n",
      "Train Batch 102/175 - Loss: 3.1728, Accuracy: 0.1562\n",
      "Train Batch 103/175 - Loss: 3.0419, Accuracy: 0.1875\n",
      "Train Batch 104/175 - Loss: 3.1236, Accuracy: 0.0938\n",
      "Train Batch 105/175 - Loss: 3.0069, Accuracy: 0.1875\n",
      "Train Batch 106/175 - Loss: 3.0924, Accuracy: 0.2188\n",
      "Train Batch 107/175 - Loss: 3.0922, Accuracy: 0.1250\n",
      "Train Batch 108/175 - Loss: 3.1164, Accuracy: 0.1250\n",
      "Train Batch 109/175 - Loss: 2.9825, Accuracy: 0.1875\n",
      "Train Batch 110/175 - Loss: 3.1826, Accuracy: 0.1250\n",
      "Train Batch 111/175 - Loss: 2.9858, Accuracy: 0.2500\n",
      "Train Batch 112/175 - Loss: 3.1825, Accuracy: 0.2500\n",
      "Train Batch 113/175 - Loss: 2.9551, Accuracy: 0.1875\n",
      "Train Batch 114/175 - Loss: 2.9927, Accuracy: 0.1250\n",
      "Train Batch 115/175 - Loss: 2.7415, Accuracy: 0.2188\n",
      "Train Batch 116/175 - Loss: 2.8352, Accuracy: 0.2188\n",
      "Train Batch 117/175 - Loss: 2.7075, Accuracy: 0.2812\n",
      "Train Batch 118/175 - Loss: 3.0796, Accuracy: 0.0625\n",
      "Train Batch 119/175 - Loss: 2.9101, Accuracy: 0.2500\n",
      "Train Batch 120/175 - Loss: 2.7848, Accuracy: 0.1562\n",
      "Train Batch 121/175 - Loss: 2.6200, Accuracy: 0.1875\n",
      "Train Batch 122/175 - Loss: 2.9512, Accuracy: 0.1562\n",
      "Train Batch 123/175 - Loss: 2.7990, Accuracy: 0.1562\n",
      "Train Batch 124/175 - Loss: 2.9544, Accuracy: 0.2188\n",
      "Train Batch 125/175 - Loss: 2.4684, Accuracy: 0.2500\n",
      "Train Batch 126/175 - Loss: 3.3423, Accuracy: 0.1250\n",
      "Train Batch 127/175 - Loss: 3.0674, Accuracy: 0.1562\n",
      "Train Batch 128/175 - Loss: 2.9533, Accuracy: 0.1250\n",
      "Train Batch 129/175 - Loss: 3.2095, Accuracy: 0.2500\n",
      "Train Batch 130/175 - Loss: 2.9532, Accuracy: 0.3750\n",
      "Train Batch 131/175 - Loss: 2.6843, Accuracy: 0.2812\n",
      "Train Batch 132/175 - Loss: 2.9157, Accuracy: 0.3125\n",
      "Train Batch 133/175 - Loss: 2.8478, Accuracy: 0.2812\n",
      "Train Batch 134/175 - Loss: 2.9736, Accuracy: 0.2188\n",
      "Train Batch 135/175 - Loss: 2.9790, Accuracy: 0.2812\n",
      "Train Batch 136/175 - Loss: 2.9463, Accuracy: 0.2500\n",
      "Train Batch 137/175 - Loss: 2.7701, Accuracy: 0.3438\n",
      "Train Batch 138/175 - Loss: 2.8567, Accuracy: 0.2188\n",
      "Train Batch 139/175 - Loss: 2.8150, Accuracy: 0.2188\n",
      "Train Batch 140/175 - Loss: 2.6641, Accuracy: 0.2812\n",
      "Train Batch 141/175 - Loss: 2.7501, Accuracy: 0.1250\n",
      "Train Batch 142/175 - Loss: 2.5781, Accuracy: 0.2812\n",
      "Train Batch 143/175 - Loss: 2.8217, Accuracy: 0.2188\n",
      "Train Batch 144/175 - Loss: 2.8764, Accuracy: 0.1562\n",
      "Train Batch 145/175 - Loss: 2.7924, Accuracy: 0.2500\n",
      "Train Batch 146/175 - Loss: 2.5542, Accuracy: 0.3438\n",
      "Train Batch 147/175 - Loss: 2.8058, Accuracy: 0.3125\n",
      "Train Batch 148/175 - Loss: 2.4506, Accuracy: 0.4062\n",
      "Train Batch 149/175 - Loss: 2.8573, Accuracy: 0.2812\n",
      "Train Batch 150/175 - Loss: 2.7670, Accuracy: 0.2188\n",
      "Train Batch 151/175 - Loss: 2.7034, Accuracy: 0.1250\n",
      "Train Batch 152/175 - Loss: 2.8273, Accuracy: 0.2812\n",
      "Train Batch 153/175 - Loss: 2.4404, Accuracy: 0.2812\n",
      "Train Batch 154/175 - Loss: 2.5413, Accuracy: 0.3125\n",
      "Train Batch 155/175 - Loss: 2.2382, Accuracy: 0.4688\n",
      "Train Batch 156/175 - Loss: 2.5867, Accuracy: 0.1562\n",
      "Train Batch 157/175 - Loss: 2.3887, Accuracy: 0.3125\n",
      "Train Batch 158/175 - Loss: 2.7615, Accuracy: 0.3750\n",
      "Train Batch 159/175 - Loss: 3.2945, Accuracy: 0.1562\n",
      "Train Batch 160/175 - Loss: 2.4875, Accuracy: 0.3125\n",
      "Train Batch 161/175 - Loss: 2.7800, Accuracy: 0.1562\n",
      "Train Batch 162/175 - Loss: 2.6810, Accuracy: 0.1875\n",
      "Train Batch 163/175 - Loss: 2.9594, Accuracy: 0.1875\n",
      "Train Batch 164/175 - Loss: 2.5457, Accuracy: 0.3125\n",
      "Train Batch 165/175 - Loss: 2.7074, Accuracy: 0.2812\n",
      "Train Batch 166/175 - Loss: 2.5419, Accuracy: 0.3438\n",
      "Train Batch 167/175 - Loss: 2.6102, Accuracy: 0.1250\n",
      "Train Batch 168/175 - Loss: 2.5378, Accuracy: 0.4062\n",
      "Train Batch 169/175 - Loss: 2.5216, Accuracy: 0.3438\n",
      "Train Batch 170/175 - Loss: 2.2767, Accuracy: 0.3750\n",
      "Train Batch 171/175 - Loss: 2.2872, Accuracy: 0.3750\n",
      "Train Batch 172/175 - Loss: 2.5480, Accuracy: 0.1875\n",
      "Train Batch 173/175 - Loss: 2.7443, Accuracy: 0.1875\n",
      "Train Batch 174/175 - Loss: 2.4084, Accuracy: 0.3750\n",
      "Train Batch 175/175 - Loss: 2.3657, Accuracy: 0.1579\n",
      "Train Epoch Loss: 3.0748 Acc: 0.1294\n",
      "Val Batch 1/44 - Loss: 2.9346, Accuracy: 0.0938\n",
      "Val Batch 2/44 - Loss: 2.9819, Accuracy: 0.2188\n",
      "Val Batch 3/44 - Loss: 2.1272, Accuracy: 0.5000\n",
      "Val Batch 4/44 - Loss: 2.1267, Accuracy: 0.4688\n",
      "Val Batch 5/44 - Loss: 2.0362, Accuracy: 0.5000\n",
      "Val Batch 6/44 - Loss: 1.7255, Accuracy: 0.6875\n",
      "Val Batch 7/44 - Loss: 2.2667, Accuracy: 0.5000\n",
      "Val Batch 8/44 - Loss: 1.4593, Accuracy: 0.8750\n",
      "Val Batch 9/44 - Loss: 2.3185, Accuracy: 0.3750\n",
      "Val Batch 10/44 - Loss: 2.8760, Accuracy: 0.0938\n",
      "Val Batch 11/44 - Loss: 2.1787, Accuracy: 0.6250\n",
      "Val Batch 12/44 - Loss: 2.1630, Accuracy: 0.5312\n",
      "Val Batch 13/44 - Loss: 1.9367, Accuracy: 0.6562\n",
      "Val Batch 14/44 - Loss: 1.9933, Accuracy: 0.6250\n",
      "Val Batch 15/44 - Loss: 2.0264, Accuracy: 0.4375\n",
      "Val Batch 16/44 - Loss: 2.5795, Accuracy: 0.1250\n",
      "Val Batch 17/44 - Loss: 2.6003, Accuracy: 0.0625\n",
      "Val Batch 18/44 - Loss: 2.2764, Accuracy: 0.4688\n",
      "Val Batch 19/44 - Loss: 2.0249, Accuracy: 0.5625\n",
      "Val Batch 20/44 - Loss: 1.2745, Accuracy: 0.8750\n",
      "Val Batch 21/44 - Loss: 1.7570, Accuracy: 0.6875\n",
      "Val Batch 22/44 - Loss: 2.0241, Accuracy: 0.4062\n",
      "Val Batch 23/44 - Loss: 1.9931, Accuracy: 0.6562\n",
      "Val Batch 24/44 - Loss: 2.1578, Accuracy: 0.5000\n",
      "Val Batch 25/44 - Loss: 2.2752, Accuracy: 0.5312\n",
      "Val Batch 26/44 - Loss: 2.2177, Accuracy: 0.2812\n",
      "Val Batch 27/44 - Loss: 2.4501, Accuracy: 0.2188\n",
      "Val Batch 28/44 - Loss: 2.8530, Accuracy: 0.1250\n",
      "Val Batch 29/44 - Loss: 2.5839, Accuracy: 0.0938\n",
      "Val Batch 30/44 - Loss: 1.9897, Accuracy: 0.6562\n",
      "Val Batch 31/44 - Loss: 1.5000, Accuracy: 0.7500\n",
      "Val Batch 32/44 - Loss: 1.4698, Accuracy: 0.8125\n",
      "Val Batch 33/44 - Loss: 1.2214, Accuracy: 0.9062\n",
      "Val Batch 34/44 - Loss: 1.2546, Accuracy: 0.9062\n",
      "Val Batch 35/44 - Loss: 3.3543, Accuracy: 0.0312\n",
      "Val Batch 36/44 - Loss: 3.2007, Accuracy: 0.0312\n",
      "Val Batch 37/44 - Loss: 2.7139, Accuracy: 0.1250\n",
      "Val Batch 38/44 - Loss: 2.4080, Accuracy: 0.1250\n",
      "Val Batch 39/44 - Loss: 2.4280, Accuracy: 0.1250\n",
      "Val Batch 40/44 - Loss: 2.4089, Accuracy: 0.4688\n",
      "Val Batch 41/44 - Loss: 2.4014, Accuracy: 0.4375\n",
      "Val Batch 42/44 - Loss: 1.9228, Accuracy: 0.6250\n",
      "Val Batch 43/44 - Loss: 2.1216, Accuracy: 0.3750\n",
      "Val Batch 44/44 - Loss: 1.9645, Accuracy: 0.3077\n",
      "Val Epoch Loss: 2.1959 Acc: 0.4429\n",
      "Epoch 2/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 2.2182, Accuracy: 0.4688\n",
      "Train Batch 2/175 - Loss: 1.8227, Accuracy: 0.4375\n",
      "Train Batch 3/175 - Loss: 2.2681, Accuracy: 0.3438\n",
      "Train Batch 4/175 - Loss: 2.0221, Accuracy: 0.5000\n",
      "Train Batch 5/175 - Loss: 2.3547, Accuracy: 0.3438\n",
      "Train Batch 6/175 - Loss: 2.0322, Accuracy: 0.3750\n",
      "Train Batch 7/175 - Loss: 2.1186, Accuracy: 0.4375\n",
      "Train Batch 8/175 - Loss: 2.9499, Accuracy: 0.2500\n",
      "Train Batch 9/175 - Loss: 2.6017, Accuracy: 0.4688\n",
      "Train Batch 10/175 - Loss: 2.1658, Accuracy: 0.4062\n",
      "Train Batch 11/175 - Loss: 2.1216, Accuracy: 0.4375\n",
      "Train Batch 12/175 - Loss: 2.4233, Accuracy: 0.3750\n",
      "Train Batch 13/175 - Loss: 1.7149, Accuracy: 0.6875\n",
      "Train Batch 14/175 - Loss: 2.3421, Accuracy: 0.5000\n",
      "Train Batch 15/175 - Loss: 1.9689, Accuracy: 0.5625\n",
      "Train Batch 16/175 - Loss: 2.3222, Accuracy: 0.2812\n",
      "Train Batch 17/175 - Loss: 2.4242, Accuracy: 0.2812\n",
      "Train Batch 18/175 - Loss: 1.8787, Accuracy: 0.4375\n",
      "Train Batch 19/175 - Loss: 2.3564, Accuracy: 0.3750\n",
      "Train Batch 20/175 - Loss: 2.2534, Accuracy: 0.2812\n",
      "Train Batch 21/175 - Loss: 2.3249, Accuracy: 0.3438\n",
      "Train Batch 22/175 - Loss: 1.9679, Accuracy: 0.5312\n",
      "Train Batch 23/175 - Loss: 2.0684, Accuracy: 0.4688\n",
      "Train Batch 24/175 - Loss: 2.0554, Accuracy: 0.3125\n",
      "Train Batch 25/175 - Loss: 2.5830, Accuracy: 0.4375\n",
      "Train Batch 26/175 - Loss: 1.8499, Accuracy: 0.5000\n",
      "Train Batch 27/175 - Loss: 1.9399, Accuracy: 0.5312\n",
      "Train Batch 28/175 - Loss: 2.2634, Accuracy: 0.2812\n",
      "Train Batch 29/175 - Loss: 1.9590, Accuracy: 0.4688\n",
      "Train Batch 30/175 - Loss: 2.0902, Accuracy: 0.4375\n",
      "Train Batch 31/175 - Loss: 1.8582, Accuracy: 0.5312\n",
      "Train Batch 32/175 - Loss: 2.1728, Accuracy: 0.3750\n",
      "Train Batch 33/175 - Loss: 2.0723, Accuracy: 0.3438\n",
      "Train Batch 34/175 - Loss: 1.5975, Accuracy: 0.5938\n",
      "Train Batch 35/175 - Loss: 2.1226, Accuracy: 0.4062\n",
      "Train Batch 36/175 - Loss: 1.9206, Accuracy: 0.4688\n",
      "Train Batch 37/175 - Loss: 1.8477, Accuracy: 0.4375\n",
      "Train Batch 38/175 - Loss: 2.1963, Accuracy: 0.5938\n",
      "Train Batch 39/175 - Loss: 1.5157, Accuracy: 0.5625\n",
      "Train Batch 40/175 - Loss: 2.3050, Accuracy: 0.5000\n",
      "Train Batch 41/175 - Loss: 1.8702, Accuracy: 0.4688\n",
      "Train Batch 42/175 - Loss: 1.7923, Accuracy: 0.5312\n",
      "Train Batch 43/175 - Loss: 1.9130, Accuracy: 0.4375\n",
      "Train Batch 44/175 - Loss: 1.9034, Accuracy: 0.5000\n",
      "Train Batch 45/175 - Loss: 1.9829, Accuracy: 0.4688\n",
      "Train Batch 46/175 - Loss: 1.5826, Accuracy: 0.5312\n",
      "Train Batch 47/175 - Loss: 1.8573, Accuracy: 0.5000\n",
      "Train Batch 48/175 - Loss: 1.8394, Accuracy: 0.5312\n",
      "Train Batch 49/175 - Loss: 1.6652, Accuracy: 0.5000\n",
      "Train Batch 50/175 - Loss: 1.6645, Accuracy: 0.5938\n",
      "Train Batch 51/175 - Loss: 1.8686, Accuracy: 0.4688\n",
      "Train Batch 52/175 - Loss: 1.4731, Accuracy: 0.6562\n",
      "Train Batch 53/175 - Loss: 1.4067, Accuracy: 0.5625\n",
      "Train Batch 54/175 - Loss: 1.6305, Accuracy: 0.4375\n",
      "Train Batch 55/175 - Loss: 1.3099, Accuracy: 0.6562\n",
      "Train Batch 56/175 - Loss: 1.7824, Accuracy: 0.5312\n",
      "Train Batch 57/175 - Loss: 1.6724, Accuracy: 0.5312\n",
      "Train Batch 58/175 - Loss: 1.3776, Accuracy: 0.6250\n",
      "Train Batch 59/175 - Loss: 2.0247, Accuracy: 0.4688\n",
      "Train Batch 60/175 - Loss: 1.9883, Accuracy: 0.5625\n",
      "Train Batch 61/175 - Loss: 1.4678, Accuracy: 0.6250\n",
      "Train Batch 62/175 - Loss: 1.5635, Accuracy: 0.5312\n",
      "Train Batch 63/175 - Loss: 1.6253, Accuracy: 0.5000\n",
      "Train Batch 64/175 - Loss: 1.1413, Accuracy: 0.7188\n",
      "Train Batch 65/175 - Loss: 1.6292, Accuracy: 0.5312\n",
      "Train Batch 66/175 - Loss: 1.9724, Accuracy: 0.5312\n",
      "Train Batch 67/175 - Loss: 1.7955, Accuracy: 0.5000\n",
      "Train Batch 68/175 - Loss: 1.3379, Accuracy: 0.6875\n",
      "Train Batch 69/175 - Loss: 1.6866, Accuracy: 0.5312\n",
      "Train Batch 70/175 - Loss: 1.5092, Accuracy: 0.5000\n",
      "Train Batch 71/175 - Loss: 1.4460, Accuracy: 0.5938\n",
      "Train Batch 72/175 - Loss: 1.4492, Accuracy: 0.5000\n",
      "Train Batch 73/175 - Loss: 1.3872, Accuracy: 0.6562\n",
      "Train Batch 74/175 - Loss: 1.5406, Accuracy: 0.5938\n",
      "Train Batch 75/175 - Loss: 1.3405, Accuracy: 0.6562\n",
      "Train Batch 76/175 - Loss: 1.6368, Accuracy: 0.5938\n",
      "Train Batch 77/175 - Loss: 1.5418, Accuracy: 0.5938\n",
      "Train Batch 78/175 - Loss: 1.0815, Accuracy: 0.6562\n",
      "Train Batch 79/175 - Loss: 1.5566, Accuracy: 0.5625\n",
      "Train Batch 80/175 - Loss: 1.2592, Accuracy: 0.6562\n",
      "Train Batch 81/175 - Loss: 1.5745, Accuracy: 0.5625\n",
      "Train Batch 82/175 - Loss: 1.3960, Accuracy: 0.6562\n",
      "Train Batch 83/175 - Loss: 1.9503, Accuracy: 0.5625\n",
      "Train Batch 84/175 - Loss: 1.4470, Accuracy: 0.5000\n",
      "Train Batch 85/175 - Loss: 1.4281, Accuracy: 0.6562\n",
      "Train Batch 86/175 - Loss: 1.1933, Accuracy: 0.6875\n",
      "Train Batch 87/175 - Loss: 1.4363, Accuracy: 0.6875\n",
      "Train Batch 88/175 - Loss: 1.9434, Accuracy: 0.5312\n",
      "Train Batch 89/175 - Loss: 1.4366, Accuracy: 0.6875\n",
      "Train Batch 90/175 - Loss: 1.7152, Accuracy: 0.6250\n",
      "Train Batch 91/175 - Loss: 1.5251, Accuracy: 0.5938\n",
      "Train Batch 92/175 - Loss: 1.0065, Accuracy: 0.7500\n",
      "Train Batch 93/175 - Loss: 1.6668, Accuracy: 0.5625\n",
      "Train Batch 94/175 - Loss: 1.3390, Accuracy: 0.5938\n",
      "Train Batch 95/175 - Loss: 1.4250, Accuracy: 0.6875\n",
      "Train Batch 96/175 - Loss: 1.4797, Accuracy: 0.5625\n",
      "Train Batch 97/175 - Loss: 1.3884, Accuracy: 0.6250\n",
      "Train Batch 98/175 - Loss: 1.3022, Accuracy: 0.6875\n",
      "Train Batch 99/175 - Loss: 1.1798, Accuracy: 0.6562\n",
      "Train Batch 100/175 - Loss: 0.8843, Accuracy: 0.7812\n",
      "Train Batch 101/175 - Loss: 1.5284, Accuracy: 0.5625\n",
      "Train Batch 102/175 - Loss: 1.4333, Accuracy: 0.5938\n",
      "Train Batch 103/175 - Loss: 1.1884, Accuracy: 0.5625\n",
      "Train Batch 104/175 - Loss: 1.4174, Accuracy: 0.5625\n",
      "Train Batch 105/175 - Loss: 1.5946, Accuracy: 0.5625\n",
      "Train Batch 106/175 - Loss: 1.1909, Accuracy: 0.6562\n",
      "Train Batch 107/175 - Loss: 0.9499, Accuracy: 0.7812\n",
      "Train Batch 108/175 - Loss: 1.2196, Accuracy: 0.5625\n",
      "Train Batch 109/175 - Loss: 1.7238, Accuracy: 0.5625\n",
      "Train Batch 110/175 - Loss: 1.0146, Accuracy: 0.7500\n",
      "Train Batch 111/175 - Loss: 1.3268, Accuracy: 0.6562\n",
      "Train Batch 112/175 - Loss: 1.1812, Accuracy: 0.6250\n",
      "Train Batch 113/175 - Loss: 1.5919, Accuracy: 0.5000\n",
      "Train Batch 114/175 - Loss: 1.0592, Accuracy: 0.6250\n",
      "Train Batch 115/175 - Loss: 1.0672, Accuracy: 0.6875\n",
      "Train Batch 116/175 - Loss: 1.7302, Accuracy: 0.5625\n",
      "Train Batch 117/175 - Loss: 1.1360, Accuracy: 0.6562\n",
      "Train Batch 118/175 - Loss: 1.2486, Accuracy: 0.6562\n",
      "Train Batch 119/175 - Loss: 1.7166, Accuracy: 0.5000\n",
      "Train Batch 120/175 - Loss: 1.7908, Accuracy: 0.5312\n",
      "Train Batch 121/175 - Loss: 1.4845, Accuracy: 0.5625\n",
      "Train Batch 122/175 - Loss: 0.9426, Accuracy: 0.7812\n",
      "Train Batch 123/175 - Loss: 0.8300, Accuracy: 0.7812\n",
      "Train Batch 124/175 - Loss: 1.1784, Accuracy: 0.6875\n",
      "Train Batch 125/175 - Loss: 0.9882, Accuracy: 0.6875\n",
      "Train Batch 126/175 - Loss: 0.8452, Accuracy: 0.6562\n",
      "Train Batch 127/175 - Loss: 1.3170, Accuracy: 0.6562\n",
      "Train Batch 128/175 - Loss: 2.1016, Accuracy: 0.4062\n",
      "Train Batch 129/175 - Loss: 0.9170, Accuracy: 0.7188\n",
      "Train Batch 130/175 - Loss: 1.5112, Accuracy: 0.6875\n",
      "Train Batch 131/175 - Loss: 0.9765, Accuracy: 0.6875\n",
      "Train Batch 132/175 - Loss: 0.9366, Accuracy: 0.7500\n",
      "Train Batch 133/175 - Loss: 1.1127, Accuracy: 0.6562\n",
      "Train Batch 134/175 - Loss: 1.2578, Accuracy: 0.6875\n",
      "Train Batch 135/175 - Loss: 1.3596, Accuracy: 0.6562\n",
      "Train Batch 136/175 - Loss: 1.3931, Accuracy: 0.5938\n",
      "Train Batch 137/175 - Loss: 0.8446, Accuracy: 0.9375\n",
      "Train Batch 138/175 - Loss: 0.9354, Accuracy: 0.6875\n",
      "Train Batch 139/175 - Loss: 1.7805, Accuracy: 0.5312\n",
      "Train Batch 140/175 - Loss: 1.6057, Accuracy: 0.6250\n",
      "Train Batch 141/175 - Loss: 0.9369, Accuracy: 0.7188\n",
      "Train Batch 142/175 - Loss: 1.0335, Accuracy: 0.6875\n",
      "Train Batch 143/175 - Loss: 1.3075, Accuracy: 0.6250\n",
      "Train Batch 144/175 - Loss: 1.2992, Accuracy: 0.6562\n",
      "Train Batch 145/175 - Loss: 1.1868, Accuracy: 0.8125\n",
      "Train Batch 146/175 - Loss: 1.2823, Accuracy: 0.7188\n",
      "Train Batch 147/175 - Loss: 1.3082, Accuracy: 0.6562\n",
      "Train Batch 148/175 - Loss: 1.7771, Accuracy: 0.5625\n",
      "Train Batch 149/175 - Loss: 1.0542, Accuracy: 0.7188\n",
      "Train Batch 150/175 - Loss: 0.8269, Accuracy: 0.7812\n",
      "Train Batch 151/175 - Loss: 0.8879, Accuracy: 0.6875\n",
      "Train Batch 152/175 - Loss: 1.4473, Accuracy: 0.5312\n",
      "Train Batch 153/175 - Loss: 0.7435, Accuracy: 0.7812\n",
      "Train Batch 154/175 - Loss: 0.7303, Accuracy: 0.7500\n",
      "Train Batch 155/175 - Loss: 0.6999, Accuracy: 0.7500\n",
      "Train Batch 156/175 - Loss: 0.9122, Accuracy: 0.6875\n",
      "Train Batch 157/175 - Loss: 1.3071, Accuracy: 0.5938\n",
      "Train Batch 158/175 - Loss: 1.3999, Accuracy: 0.6875\n",
      "Train Batch 159/175 - Loss: 1.1781, Accuracy: 0.7812\n",
      "Train Batch 160/175 - Loss: 1.1456, Accuracy: 0.6562\n",
      "Train Batch 161/175 - Loss: 1.1070, Accuracy: 0.6875\n",
      "Train Batch 162/175 - Loss: 1.1762, Accuracy: 0.5938\n",
      "Train Batch 163/175 - Loss: 0.9007, Accuracy: 0.6875\n",
      "Train Batch 164/175 - Loss: 1.0309, Accuracy: 0.7812\n",
      "Train Batch 165/175 - Loss: 0.7558, Accuracy: 0.7500\n",
      "Train Batch 166/175 - Loss: 0.7980, Accuracy: 0.7188\n",
      "Train Batch 167/175 - Loss: 1.1906, Accuracy: 0.6875\n",
      "Train Batch 168/175 - Loss: 1.0433, Accuracy: 0.6562\n",
      "Train Batch 169/175 - Loss: 0.7598, Accuracy: 0.7812\n",
      "Train Batch 170/175 - Loss: 1.1254, Accuracy: 0.7188\n",
      "Train Batch 171/175 - Loss: 0.7630, Accuracy: 0.8125\n",
      "Train Batch 172/175 - Loss: 1.1176, Accuracy: 0.7500\n",
      "Train Batch 173/175 - Loss: 0.8420, Accuracy: 0.8125\n",
      "Train Batch 174/175 - Loss: 1.4284, Accuracy: 0.7500\n",
      "Train Batch 175/175 - Loss: 2.1938, Accuracy: 0.4737\n",
      "Train Epoch Loss: 1.5327 Acc: 0.5855\n",
      "Val Batch 1/44 - Loss: 1.4774, Accuracy: 0.6250\n",
      "Val Batch 2/44 - Loss: 1.4474, Accuracy: 0.6250\n",
      "Val Batch 3/44 - Loss: 0.2406, Accuracy: 0.9062\n",
      "Val Batch 4/44 - Loss: 0.7231, Accuracy: 0.8438\n",
      "Val Batch 5/44 - Loss: 0.2820, Accuracy: 0.9375\n",
      "Val Batch 6/44 - Loss: 1.0208, Accuracy: 0.6250\n",
      "Val Batch 7/44 - Loss: 1.5273, Accuracy: 0.5000\n",
      "Val Batch 8/44 - Loss: 0.4724, Accuracy: 0.8750\n",
      "Val Batch 9/44 - Loss: 1.0993, Accuracy: 0.7188\n",
      "Val Batch 10/44 - Loss: 1.5163, Accuracy: 0.6250\n",
      "Val Batch 11/44 - Loss: 0.7195, Accuracy: 0.8750\n",
      "Val Batch 12/44 - Loss: 1.0885, Accuracy: 0.6875\n",
      "Val Batch 13/44 - Loss: 0.2967, Accuracy: 0.9688\n",
      "Val Batch 14/44 - Loss: 0.5829, Accuracy: 0.8750\n",
      "Val Batch 15/44 - Loss: 1.0049, Accuracy: 0.7188\n",
      "Val Batch 16/44 - Loss: 1.5656, Accuracy: 0.5938\n",
      "Val Batch 17/44 - Loss: 1.2886, Accuracy: 0.6250\n",
      "Val Batch 18/44 - Loss: 0.7612, Accuracy: 0.7500\n",
      "Val Batch 19/44 - Loss: 0.6285, Accuracy: 0.7812\n",
      "Val Batch 20/44 - Loss: 0.2202, Accuracy: 0.9688\n",
      "Val Batch 21/44 - Loss: 0.7375, Accuracy: 0.7500\n",
      "Val Batch 22/44 - Loss: 1.2252, Accuracy: 0.6562\n",
      "Val Batch 23/44 - Loss: 0.6320, Accuracy: 0.9062\n",
      "Val Batch 24/44 - Loss: 0.5853, Accuracy: 0.9062\n",
      "Val Batch 25/44 - Loss: 0.8206, Accuracy: 0.8750\n",
      "Val Batch 26/44 - Loss: 0.6201, Accuracy: 0.9062\n",
      "Val Batch 27/44 - Loss: 0.5849, Accuracy: 0.9062\n",
      "Val Batch 28/44 - Loss: 1.3950, Accuracy: 0.6875\n",
      "Val Batch 29/44 - Loss: 0.9445, Accuracy: 0.7812\n",
      "Val Batch 30/44 - Loss: 0.5793, Accuracy: 0.9062\n",
      "Val Batch 31/44 - Loss: 0.4073, Accuracy: 0.9375\n",
      "Val Batch 32/44 - Loss: 0.6605, Accuracy: 0.9062\n",
      "Val Batch 33/44 - Loss: 0.3739, Accuracy: 0.9062\n",
      "Val Batch 34/44 - Loss: 0.1676, Accuracy: 1.0000\n",
      "Val Batch 35/44 - Loss: 0.7887, Accuracy: 0.8125\n",
      "Val Batch 36/44 - Loss: 0.8161, Accuracy: 0.7188\n",
      "Val Batch 37/44 - Loss: 1.3807, Accuracy: 0.5625\n",
      "Val Batch 38/44 - Loss: 0.8188, Accuracy: 0.8125\n",
      "Val Batch 39/44 - Loss: 0.7500, Accuracy: 0.8438\n",
      "Val Batch 40/44 - Loss: 1.0972, Accuracy: 0.7188\n",
      "Val Batch 41/44 - Loss: 1.0244, Accuracy: 0.8125\n",
      "Val Batch 42/44 - Loss: 0.5156, Accuracy: 0.9062\n",
      "Val Batch 43/44 - Loss: 0.8790, Accuracy: 0.7812\n",
      "Val Batch 44/44 - Loss: 0.8211, Accuracy: 0.7692\n",
      "Val Epoch Loss: 0.8316 Acc: 0.7932\n",
      "Epoch 3/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 1.0124, Accuracy: 0.7812\n",
      "Train Batch 2/175 - Loss: 1.1961, Accuracy: 0.7500\n",
      "Train Batch 3/175 - Loss: 0.8343, Accuracy: 0.8125\n",
      "Train Batch 4/175 - Loss: 1.1194, Accuracy: 0.6250\n",
      "Train Batch 5/175 - Loss: 1.1542, Accuracy: 0.6875\n",
      "Train Batch 6/175 - Loss: 0.5765, Accuracy: 0.8750\n",
      "Train Batch 7/175 - Loss: 1.0580, Accuracy: 0.6562\n",
      "Train Batch 8/175 - Loss: 0.6835, Accuracy: 0.8125\n",
      "Train Batch 9/175 - Loss: 1.1800, Accuracy: 0.6875\n",
      "Train Batch 10/175 - Loss: 1.0041, Accuracy: 0.7188\n",
      "Train Batch 11/175 - Loss: 1.2102, Accuracy: 0.7188\n",
      "Train Batch 12/175 - Loss: 0.8326, Accuracy: 0.7188\n",
      "Train Batch 13/175 - Loss: 1.1556, Accuracy: 0.5625\n",
      "Train Batch 14/175 - Loss: 0.9807, Accuracy: 0.7500\n",
      "Train Batch 15/175 - Loss: 0.6911, Accuracy: 0.7500\n",
      "Train Batch 16/175 - Loss: 1.0137, Accuracy: 0.6875\n",
      "Train Batch 17/175 - Loss: 0.7852, Accuracy: 0.7500\n",
      "Train Batch 18/175 - Loss: 0.8768, Accuracy: 0.7188\n",
      "Train Batch 19/175 - Loss: 0.7382, Accuracy: 0.8438\n",
      "Train Batch 20/175 - Loss: 1.0634, Accuracy: 0.6875\n",
      "Train Batch 21/175 - Loss: 1.2388, Accuracy: 0.6562\n",
      "Train Batch 22/175 - Loss: 0.7196, Accuracy: 0.8125\n",
      "Train Batch 23/175 - Loss: 0.6283, Accuracy: 0.8750\n",
      "Train Batch 24/175 - Loss: 0.5696, Accuracy: 0.8438\n",
      "Train Batch 25/175 - Loss: 0.5002, Accuracy: 0.8125\n",
      "Train Batch 26/175 - Loss: 1.1105, Accuracy: 0.8125\n",
      "Train Batch 27/175 - Loss: 0.8083, Accuracy: 0.6562\n",
      "Train Batch 28/175 - Loss: 1.4619, Accuracy: 0.6562\n",
      "Train Batch 29/175 - Loss: 0.9246, Accuracy: 0.6562\n",
      "Train Batch 30/175 - Loss: 0.6613, Accuracy: 0.6875\n",
      "Train Batch 31/175 - Loss: 0.4632, Accuracy: 0.8750\n",
      "Train Batch 32/175 - Loss: 1.2180, Accuracy: 0.6562\n",
      "Train Batch 33/175 - Loss: 0.7987, Accuracy: 0.8750\n",
      "Train Batch 34/175 - Loss: 0.7078, Accuracy: 0.8125\n",
      "Train Batch 35/175 - Loss: 0.7887, Accuracy: 0.7500\n",
      "Train Batch 36/175 - Loss: 0.8597, Accuracy: 0.8125\n",
      "Train Batch 37/175 - Loss: 0.7244, Accuracy: 0.8438\n",
      "Train Batch 38/175 - Loss: 0.5635, Accuracy: 0.8750\n",
      "Train Batch 39/175 - Loss: 0.2121, Accuracy: 0.9688\n",
      "Train Batch 40/175 - Loss: 0.7693, Accuracy: 0.8125\n",
      "Train Batch 41/175 - Loss: 0.9508, Accuracy: 0.7500\n",
      "Train Batch 42/175 - Loss: 0.2274, Accuracy: 0.9375\n",
      "Train Batch 43/175 - Loss: 0.7581, Accuracy: 0.7500\n",
      "Train Batch 44/175 - Loss: 0.6543, Accuracy: 0.7500\n",
      "Train Batch 45/175 - Loss: 0.8944, Accuracy: 0.8125\n",
      "Train Batch 46/175 - Loss: 1.0500, Accuracy: 0.6562\n",
      "Train Batch 47/175 - Loss: 0.9316, Accuracy: 0.8125\n",
      "Train Batch 48/175 - Loss: 0.7884, Accuracy: 0.7500\n",
      "Train Batch 49/175 - Loss: 1.0139, Accuracy: 0.7500\n",
      "Train Batch 50/175 - Loss: 0.9832, Accuracy: 0.6875\n",
      "Train Batch 51/175 - Loss: 0.5699, Accuracy: 0.8750\n",
      "Train Batch 52/175 - Loss: 0.6947, Accuracy: 0.7500\n",
      "Train Batch 53/175 - Loss: 0.9326, Accuracy: 0.6250\n",
      "Train Batch 54/175 - Loss: 0.6727, Accuracy: 0.7188\n",
      "Train Batch 55/175 - Loss: 0.8077, Accuracy: 0.8438\n",
      "Train Batch 56/175 - Loss: 1.1414, Accuracy: 0.7500\n",
      "Train Batch 57/175 - Loss: 0.6452, Accuracy: 0.7500\n",
      "Train Batch 58/175 - Loss: 0.9353, Accuracy: 0.7500\n",
      "Train Batch 59/175 - Loss: 0.9179, Accuracy: 0.6875\n",
      "Train Batch 60/175 - Loss: 0.9259, Accuracy: 0.7812\n",
      "Train Batch 61/175 - Loss: 0.8467, Accuracy: 0.7500\n",
      "Train Batch 62/175 - Loss: 1.0605, Accuracy: 0.7188\n",
      "Train Batch 63/175 - Loss: 0.8274, Accuracy: 0.8438\n",
      "Train Batch 64/175 - Loss: 0.8420, Accuracy: 0.8125\n",
      "Train Batch 65/175 - Loss: 1.0295, Accuracy: 0.6562\n",
      "Train Batch 66/175 - Loss: 0.7180, Accuracy: 0.7812\n",
      "Train Batch 67/175 - Loss: 1.1272, Accuracy: 0.6562\n",
      "Train Batch 68/175 - Loss: 0.7462, Accuracy: 0.8438\n",
      "Train Batch 69/175 - Loss: 0.5772, Accuracy: 0.8125\n",
      "Train Batch 70/175 - Loss: 0.8881, Accuracy: 0.7812\n",
      "Train Batch 71/175 - Loss: 0.4890, Accuracy: 0.8438\n",
      "Train Batch 72/175 - Loss: 0.6492, Accuracy: 0.8438\n",
      "Train Batch 73/175 - Loss: 0.7731, Accuracy: 0.8438\n",
      "Train Batch 74/175 - Loss: 1.3043, Accuracy: 0.6562\n",
      "Train Batch 75/175 - Loss: 0.4144, Accuracy: 0.8125\n",
      "Train Batch 76/175 - Loss: 0.7515, Accuracy: 0.7500\n",
      "Train Batch 77/175 - Loss: 0.7524, Accuracy: 0.7500\n",
      "Train Batch 78/175 - Loss: 0.5717, Accuracy: 0.8438\n",
      "Train Batch 79/175 - Loss: 0.8591, Accuracy: 0.8125\n",
      "Train Batch 80/175 - Loss: 0.7093, Accuracy: 0.6875\n",
      "Train Batch 81/175 - Loss: 0.7433, Accuracy: 0.7500\n",
      "Train Batch 82/175 - Loss: 0.8895, Accuracy: 0.6875\n",
      "Train Batch 83/175 - Loss: 1.4371, Accuracy: 0.6875\n",
      "Train Batch 84/175 - Loss: 0.6583, Accuracy: 0.7812\n",
      "Train Batch 85/175 - Loss: 0.9410, Accuracy: 0.7500\n",
      "Train Batch 86/175 - Loss: 0.8147, Accuracy: 0.6875\n",
      "Train Batch 87/175 - Loss: 0.6148, Accuracy: 0.7500\n",
      "Train Batch 88/175 - Loss: 0.5340, Accuracy: 0.8438\n",
      "Train Batch 89/175 - Loss: 0.9931, Accuracy: 0.7188\n",
      "Train Batch 90/175 - Loss: 0.9893, Accuracy: 0.7188\n",
      "Train Batch 91/175 - Loss: 0.5748, Accuracy: 0.8438\n",
      "Train Batch 92/175 - Loss: 0.5125, Accuracy: 0.8438\n",
      "Train Batch 93/175 - Loss: 0.5411, Accuracy: 0.8750\n",
      "Train Batch 94/175 - Loss: 0.7758, Accuracy: 0.7500\n",
      "Train Batch 95/175 - Loss: 1.1533, Accuracy: 0.7500\n",
      "Train Batch 96/175 - Loss: 0.4354, Accuracy: 0.9062\n",
      "Train Batch 97/175 - Loss: 0.7592, Accuracy: 0.7188\n",
      "Train Batch 98/175 - Loss: 1.3183, Accuracy: 0.6250\n",
      "Train Batch 99/175 - Loss: 0.8375, Accuracy: 0.7188\n",
      "Train Batch 100/175 - Loss: 0.4041, Accuracy: 0.8750\n",
      "Train Batch 101/175 - Loss: 0.5850, Accuracy: 0.8125\n",
      "Train Batch 102/175 - Loss: 0.6274, Accuracy: 0.7500\n",
      "Train Batch 103/175 - Loss: 0.7524, Accuracy: 0.7500\n",
      "Train Batch 104/175 - Loss: 0.8273, Accuracy: 0.7500\n",
      "Train Batch 105/175 - Loss: 0.4629, Accuracy: 0.9062\n",
      "Train Batch 106/175 - Loss: 0.9806, Accuracy: 0.7500\n",
      "Train Batch 107/175 - Loss: 0.5577, Accuracy: 0.8438\n",
      "Train Batch 108/175 - Loss: 0.8461, Accuracy: 0.7500\n",
      "Train Batch 109/175 - Loss: 1.2283, Accuracy: 0.6875\n",
      "Train Batch 110/175 - Loss: 0.8233, Accuracy: 0.7812\n",
      "Train Batch 111/175 - Loss: 0.6406, Accuracy: 0.8125\n",
      "Train Batch 112/175 - Loss: 0.4704, Accuracy: 0.8125\n",
      "Train Batch 113/175 - Loss: 0.7712, Accuracy: 0.7188\n",
      "Train Batch 114/175 - Loss: 0.7061, Accuracy: 0.7500\n",
      "Train Batch 115/175 - Loss: 0.3231, Accuracy: 0.9062\n",
      "Train Batch 116/175 - Loss: 0.6379, Accuracy: 0.8125\n",
      "Train Batch 117/175 - Loss: 0.9224, Accuracy: 0.7500\n",
      "Train Batch 118/175 - Loss: 0.6997, Accuracy: 0.8125\n",
      "Train Batch 119/175 - Loss: 0.5353, Accuracy: 0.8750\n",
      "Train Batch 120/175 - Loss: 0.7572, Accuracy: 0.8125\n",
      "Train Batch 121/175 - Loss: 0.8861, Accuracy: 0.7500\n",
      "Train Batch 122/175 - Loss: 0.4994, Accuracy: 0.8438\n",
      "Train Batch 123/175 - Loss: 0.6605, Accuracy: 0.8438\n",
      "Train Batch 124/175 - Loss: 0.8800, Accuracy: 0.7812\n",
      "Train Batch 125/175 - Loss: 0.5434, Accuracy: 0.7500\n",
      "Train Batch 126/175 - Loss: 0.9698, Accuracy: 0.7188\n",
      "Train Batch 127/175 - Loss: 0.3385, Accuracy: 0.9375\n",
      "Train Batch 128/175 - Loss: 0.7566, Accuracy: 0.7188\n",
      "Train Batch 129/175 - Loss: 0.8365, Accuracy: 0.8125\n",
      "Train Batch 130/175 - Loss: 0.6855, Accuracy: 0.8438\n",
      "Train Batch 131/175 - Loss: 0.9158, Accuracy: 0.8125\n",
      "Train Batch 132/175 - Loss: 0.6083, Accuracy: 0.7188\n",
      "Train Batch 133/175 - Loss: 0.2658, Accuracy: 0.9688\n",
      "Train Batch 134/175 - Loss: 0.7230, Accuracy: 0.8125\n",
      "Train Batch 135/175 - Loss: 0.8263, Accuracy: 0.8125\n",
      "Train Batch 136/175 - Loss: 0.4228, Accuracy: 0.8750\n",
      "Train Batch 137/175 - Loss: 0.8552, Accuracy: 0.6875\n",
      "Train Batch 138/175 - Loss: 0.9013, Accuracy: 0.7500\n",
      "Train Batch 139/175 - Loss: 0.5252, Accuracy: 0.8438\n",
      "Train Batch 140/175 - Loss: 0.6822, Accuracy: 0.7812\n",
      "Train Batch 141/175 - Loss: 0.9047, Accuracy: 0.7500\n",
      "Train Batch 142/175 - Loss: 0.6180, Accuracy: 0.8125\n",
      "Train Batch 143/175 - Loss: 0.6596, Accuracy: 0.7500\n",
      "Train Batch 144/175 - Loss: 0.5524, Accuracy: 0.8125\n",
      "Train Batch 145/175 - Loss: 0.9976, Accuracy: 0.7500\n",
      "Train Batch 146/175 - Loss: 0.7770, Accuracy: 0.8125\n",
      "Train Batch 147/175 - Loss: 0.8631, Accuracy: 0.7500\n",
      "Train Batch 148/175 - Loss: 0.2338, Accuracy: 0.9062\n",
      "Train Batch 149/175 - Loss: 0.3916, Accuracy: 0.8750\n",
      "Train Batch 150/175 - Loss: 0.4209, Accuracy: 0.9062\n",
      "Train Batch 151/175 - Loss: 0.2633, Accuracy: 0.9375\n",
      "Train Batch 152/175 - Loss: 0.3976, Accuracy: 0.8750\n",
      "Train Batch 153/175 - Loss: 0.4605, Accuracy: 0.8750\n",
      "Train Batch 154/175 - Loss: 0.5348, Accuracy: 0.8438\n",
      "Train Batch 155/175 - Loss: 1.0363, Accuracy: 0.6875\n",
      "Train Batch 156/175 - Loss: 1.0450, Accuracy: 0.8438\n",
      "Train Batch 157/175 - Loss: 0.6668, Accuracy: 0.7812\n",
      "Train Batch 158/175 - Loss: 1.1131, Accuracy: 0.7188\n",
      "Train Batch 159/175 - Loss: 0.3966, Accuracy: 0.9062\n",
      "Train Batch 160/175 - Loss: 0.6037, Accuracy: 0.9062\n",
      "Train Batch 161/175 - Loss: 0.5754, Accuracy: 0.8750\n",
      "Train Batch 162/175 - Loss: 0.3835, Accuracy: 0.8750\n",
      "Train Batch 163/175 - Loss: 0.5534, Accuracy: 0.8750\n",
      "Train Batch 164/175 - Loss: 0.4519, Accuracy: 0.8750\n",
      "Train Batch 165/175 - Loss: 0.3997, Accuracy: 0.8438\n",
      "Train Batch 166/175 - Loss: 0.7613, Accuracy: 0.7500\n",
      "Train Batch 167/175 - Loss: 0.4340, Accuracy: 0.8438\n",
      "Train Batch 168/175 - Loss: 0.8166, Accuracy: 0.7812\n",
      "Train Batch 169/175 - Loss: 0.7393, Accuracy: 0.8125\n",
      "Train Batch 170/175 - Loss: 0.9460, Accuracy: 0.8125\n",
      "Train Batch 171/175 - Loss: 0.7848, Accuracy: 0.7500\n",
      "Train Batch 172/175 - Loss: 1.0600, Accuracy: 0.7500\n",
      "Train Batch 173/175 - Loss: 0.5525, Accuracy: 0.8750\n",
      "Train Batch 174/175 - Loss: 0.4255, Accuracy: 0.9062\n",
      "Train Batch 175/175 - Loss: 0.6830, Accuracy: 0.7895\n",
      "Train Epoch Loss: 0.7642 Acc: 0.7836\n",
      "Val Batch 1/44 - Loss: 0.2522, Accuracy: 0.9688\n",
      "Val Batch 2/44 - Loss: 0.5670, Accuracy: 0.9062\n",
      "Val Batch 3/44 - Loss: 0.2177, Accuracy: 0.9062\n",
      "Val Batch 4/44 - Loss: 0.7481, Accuracy: 0.8438\n",
      "Val Batch 5/44 - Loss: 0.3826, Accuracy: 0.9375\n",
      "Val Batch 6/44 - Loss: 0.8628, Accuracy: 0.7500\n",
      "Val Batch 7/44 - Loss: 0.8680, Accuracy: 0.7500\n",
      "Val Batch 8/44 - Loss: 0.2071, Accuracy: 0.9688\n",
      "Val Batch 9/44 - Loss: 0.4279, Accuracy: 0.8438\n",
      "Val Batch 10/44 - Loss: 0.5855, Accuracy: 0.9062\n",
      "Val Batch 11/44 - Loss: 0.3246, Accuracy: 0.9375\n",
      "Val Batch 12/44 - Loss: 0.6015, Accuracy: 0.8438\n",
      "Val Batch 13/44 - Loss: 0.0439, Accuracy: 1.0000\n",
      "Val Batch 14/44 - Loss: 0.2184, Accuracy: 0.9688\n",
      "Val Batch 15/44 - Loss: 0.5433, Accuracy: 0.8438\n",
      "Val Batch 16/44 - Loss: 0.9302, Accuracy: 0.6562\n",
      "Val Batch 17/44 - Loss: 0.4780, Accuracy: 0.8438\n",
      "Val Batch 18/44 - Loss: 0.3608, Accuracy: 0.8438\n",
      "Val Batch 19/44 - Loss: 0.4856, Accuracy: 0.8125\n",
      "Val Batch 20/44 - Loss: 0.3496, Accuracy: 0.8438\n",
      "Val Batch 21/44 - Loss: 0.2469, Accuracy: 1.0000\n",
      "Val Batch 22/44 - Loss: 0.6117, Accuracy: 0.9062\n",
      "Val Batch 23/44 - Loss: 0.5721, Accuracy: 0.9062\n",
      "Val Batch 24/44 - Loss: 0.6555, Accuracy: 0.8125\n",
      "Val Batch 25/44 - Loss: 0.5600, Accuracy: 0.8750\n",
      "Val Batch 26/44 - Loss: 0.5033, Accuracy: 0.8750\n",
      "Val Batch 27/44 - Loss: 0.4732, Accuracy: 0.8750\n",
      "Val Batch 28/44 - Loss: 0.4697, Accuracy: 0.9062\n",
      "Val Batch 29/44 - Loss: 0.5498, Accuracy: 0.8125\n",
      "Val Batch 30/44 - Loss: 0.2747, Accuracy: 0.9375\n",
      "Val Batch 31/44 - Loss: 0.2934, Accuracy: 0.9688\n",
      "Val Batch 32/44 - Loss: 0.2930, Accuracy: 0.9375\n",
      "Val Batch 33/44 - Loss: 0.2383, Accuracy: 0.9375\n",
      "Val Batch 34/44 - Loss: 0.1722, Accuracy: 1.0000\n",
      "Val Batch 35/44 - Loss: 0.6414, Accuracy: 0.7812\n",
      "Val Batch 36/44 - Loss: 0.4858, Accuracy: 0.8750\n",
      "Val Batch 37/44 - Loss: 1.1234, Accuracy: 0.6875\n",
      "Val Batch 38/44 - Loss: 0.5807, Accuracy: 0.8750\n",
      "Val Batch 39/44 - Loss: 0.5254, Accuracy: 0.8125\n",
      "Val Batch 40/44 - Loss: 0.3947, Accuracy: 0.8438\n",
      "Val Batch 41/44 - Loss: 0.6499, Accuracy: 0.8750\n",
      "Val Batch 42/44 - Loss: 0.4625, Accuracy: 0.8438\n",
      "Val Batch 43/44 - Loss: 0.3709, Accuracy: 0.8750\n",
      "Val Batch 44/44 - Loss: 0.2590, Accuracy: 0.9615\n",
      "Val Epoch Loss: 0.4751 Acc: 0.8759\n",
      "Epoch 4/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 0.4391, Accuracy: 0.9062\n",
      "Train Batch 2/175 - Loss: 0.3116, Accuracy: 0.9062\n",
      "Train Batch 3/175 - Loss: 0.8041, Accuracy: 0.8750\n",
      "Train Batch 4/175 - Loss: 0.6474, Accuracy: 0.8125\n",
      "Train Batch 5/175 - Loss: 0.8824, Accuracy: 0.8438\n",
      "Train Batch 6/175 - Loss: 0.9088, Accuracy: 0.7500\n",
      "Train Batch 7/175 - Loss: 0.3508, Accuracy: 0.9688\n",
      "Train Batch 8/175 - Loss: 0.3805, Accuracy: 0.9062\n",
      "Train Batch 9/175 - Loss: 0.4826, Accuracy: 0.8125\n",
      "Train Batch 10/175 - Loss: 0.4771, Accuracy: 0.8438\n",
      "Train Batch 11/175 - Loss: 0.3170, Accuracy: 0.9062\n",
      "Train Batch 12/175 - Loss: 0.2660, Accuracy: 0.9688\n",
      "Train Batch 13/175 - Loss: 0.5884, Accuracy: 0.8438\n",
      "Train Batch 14/175 - Loss: 0.6984, Accuracy: 0.7500\n",
      "Train Batch 15/175 - Loss: 0.7067, Accuracy: 0.7812\n",
      "Train Batch 16/175 - Loss: 0.3264, Accuracy: 0.9375\n",
      "Train Batch 17/175 - Loss: 0.4594, Accuracy: 0.9062\n",
      "Train Batch 18/175 - Loss: 0.4282, Accuracy: 0.8125\n",
      "Train Batch 19/175 - Loss: 0.4731, Accuracy: 0.9062\n",
      "Train Batch 20/175 - Loss: 0.4202, Accuracy: 0.8750\n",
      "Train Batch 21/175 - Loss: 0.2373, Accuracy: 0.9688\n",
      "Train Batch 22/175 - Loss: 0.3567, Accuracy: 0.9062\n",
      "Train Batch 23/175 - Loss: 0.4310, Accuracy: 0.9375\n",
      "Train Batch 24/175 - Loss: 0.9781, Accuracy: 0.7500\n",
      "Train Batch 25/175 - Loss: 0.3150, Accuracy: 0.9062\n",
      "Train Batch 26/175 - Loss: 0.8134, Accuracy: 0.8438\n",
      "Train Batch 27/175 - Loss: 0.6233, Accuracy: 0.8125\n",
      "Train Batch 28/175 - Loss: 0.4006, Accuracy: 0.8750\n",
      "Train Batch 29/175 - Loss: 0.4044, Accuracy: 0.8438\n",
      "Train Batch 30/175 - Loss: 0.5749, Accuracy: 0.8438\n",
      "Train Batch 31/175 - Loss: 0.4686, Accuracy: 0.8750\n",
      "Train Batch 32/175 - Loss: 0.3498, Accuracy: 0.8125\n",
      "Train Batch 33/175 - Loss: 0.4130, Accuracy: 0.8438\n",
      "Train Batch 34/175 - Loss: 0.1196, Accuracy: 1.0000\n",
      "Train Batch 35/175 - Loss: 0.2445, Accuracy: 0.9062\n",
      "Train Batch 36/175 - Loss: 0.6788, Accuracy: 0.7500\n",
      "Train Batch 37/175 - Loss: 0.7067, Accuracy: 0.8438\n",
      "Train Batch 38/175 - Loss: 0.1334, Accuracy: 0.9688\n",
      "Train Batch 39/175 - Loss: 0.2997, Accuracy: 0.9062\n",
      "Train Batch 40/175 - Loss: 0.3993, Accuracy: 0.9375\n",
      "Train Batch 41/175 - Loss: 0.2412, Accuracy: 0.9375\n",
      "Train Batch 42/175 - Loss: 0.6829, Accuracy: 0.8125\n",
      "Train Batch 43/175 - Loss: 0.2609, Accuracy: 0.9375\n",
      "Train Batch 44/175 - Loss: 0.4971, Accuracy: 0.8750\n",
      "Train Batch 45/175 - Loss: 0.4301, Accuracy: 0.9062\n",
      "Train Batch 46/175 - Loss: 0.3666, Accuracy: 0.9062\n",
      "Train Batch 47/175 - Loss: 0.5167, Accuracy: 0.9375\n",
      "Train Batch 48/175 - Loss: 0.2414, Accuracy: 0.8750\n",
      "Train Batch 49/175 - Loss: 0.4107, Accuracy: 0.8750\n",
      "Train Batch 50/175 - Loss: 0.3797, Accuracy: 0.9375\n",
      "Train Batch 51/175 - Loss: 0.3947, Accuracy: 0.8438\n",
      "Train Batch 52/175 - Loss: 0.3743, Accuracy: 0.9062\n",
      "Train Batch 53/175 - Loss: 0.6237, Accuracy: 0.7188\n",
      "Train Batch 54/175 - Loss: 0.5571, Accuracy: 0.8438\n",
      "Train Batch 55/175 - Loss: 0.3652, Accuracy: 0.9375\n",
      "Train Batch 56/175 - Loss: 0.5775, Accuracy: 0.8750\n",
      "Train Batch 57/175 - Loss: 0.5387, Accuracy: 0.8125\n",
      "Train Batch 58/175 - Loss: 0.3114, Accuracy: 0.9375\n",
      "Train Batch 59/175 - Loss: 0.7592, Accuracy: 0.8750\n",
      "Train Batch 60/175 - Loss: 0.3761, Accuracy: 0.8750\n",
      "Train Batch 61/175 - Loss: 0.0996, Accuracy: 1.0000\n",
      "Train Batch 62/175 - Loss: 0.5583, Accuracy: 0.8438\n",
      "Train Batch 63/175 - Loss: 0.5509, Accuracy: 0.9062\n",
      "Train Batch 64/175 - Loss: 0.5194, Accuracy: 0.8438\n",
      "Train Batch 65/175 - Loss: 0.5516, Accuracy: 0.8438\n",
      "Train Batch 66/175 - Loss: 0.2577, Accuracy: 0.9062\n",
      "Train Batch 67/175 - Loss: 0.6958, Accuracy: 0.7500\n",
      "Train Batch 68/175 - Loss: 0.4134, Accuracy: 0.8750\n",
      "Train Batch 69/175 - Loss: 0.2530, Accuracy: 0.9375\n",
      "Train Batch 70/175 - Loss: 0.3241, Accuracy: 0.8750\n",
      "Train Batch 71/175 - Loss: 0.7597, Accuracy: 0.8438\n",
      "Train Batch 72/175 - Loss: 0.4465, Accuracy: 0.8750\n",
      "Train Batch 73/175 - Loss: 0.3427, Accuracy: 0.9375\n",
      "Train Batch 74/175 - Loss: 0.3912, Accuracy: 0.9062\n",
      "Train Batch 75/175 - Loss: 0.3056, Accuracy: 0.9375\n",
      "Train Batch 76/175 - Loss: 0.4732, Accuracy: 0.8750\n",
      "Train Batch 77/175 - Loss: 0.5644, Accuracy: 0.8125\n",
      "Train Batch 78/175 - Loss: 0.5268, Accuracy: 0.8438\n",
      "Train Batch 79/175 - Loss: 0.4073, Accuracy: 0.9062\n",
      "Train Batch 80/175 - Loss: 0.3384, Accuracy: 0.9375\n",
      "Train Batch 81/175 - Loss: 0.5605, Accuracy: 0.7500\n",
      "Train Batch 82/175 - Loss: 0.4978, Accuracy: 0.8438\n",
      "Train Batch 83/175 - Loss: 0.3939, Accuracy: 0.9375\n",
      "Train Batch 84/175 - Loss: 0.3461, Accuracy: 0.9375\n",
      "Train Batch 85/175 - Loss: 0.3934, Accuracy: 0.8750\n",
      "Train Batch 86/175 - Loss: 0.4320, Accuracy: 0.9375\n",
      "Train Batch 87/175 - Loss: 0.2335, Accuracy: 0.9375\n",
      "Train Batch 88/175 - Loss: 0.3317, Accuracy: 0.9062\n",
      "Train Batch 89/175 - Loss: 0.4370, Accuracy: 0.8750\n",
      "Train Batch 90/175 - Loss: 0.4792, Accuracy: 0.8438\n",
      "Train Batch 91/175 - Loss: 0.3867, Accuracy: 0.9062\n",
      "Train Batch 92/175 - Loss: 0.4315, Accuracy: 0.9062\n",
      "Train Batch 93/175 - Loss: 0.7196, Accuracy: 0.8125\n",
      "Train Batch 94/175 - Loss: 0.2517, Accuracy: 0.9375\n",
      "Train Batch 95/175 - Loss: 0.6420, Accuracy: 0.8125\n",
      "Train Batch 96/175 - Loss: 0.3002, Accuracy: 0.9062\n",
      "Train Batch 97/175 - Loss: 0.3783, Accuracy: 0.8438\n",
      "Train Batch 98/175 - Loss: 0.3688, Accuracy: 0.8750\n",
      "Train Batch 99/175 - Loss: 0.3935, Accuracy: 0.8438\n",
      "Train Batch 100/175 - Loss: 0.7148, Accuracy: 0.7500\n",
      "Train Batch 101/175 - Loss: 0.1661, Accuracy: 0.9688\n",
      "Train Batch 102/175 - Loss: 0.4664, Accuracy: 0.8438\n",
      "Train Batch 103/175 - Loss: 0.2626, Accuracy: 0.8750\n",
      "Train Batch 104/175 - Loss: 0.3022, Accuracy: 0.9688\n",
      "Train Batch 105/175 - Loss: 0.1986, Accuracy: 0.9375\n",
      "Train Batch 106/175 - Loss: 0.2996, Accuracy: 0.9062\n",
      "Train Batch 107/175 - Loss: 0.3635, Accuracy: 0.8750\n",
      "Train Batch 108/175 - Loss: 0.5027, Accuracy: 0.8438\n",
      "Train Batch 109/175 - Loss: 0.1382, Accuracy: 0.9375\n",
      "Train Batch 110/175 - Loss: 0.5032, Accuracy: 0.9062\n",
      "Train Batch 111/175 - Loss: 0.8298, Accuracy: 0.7812\n",
      "Train Batch 112/175 - Loss: 0.5094, Accuracy: 0.8125\n",
      "Train Batch 113/175 - Loss: 0.3679, Accuracy: 0.7812\n",
      "Train Batch 114/175 - Loss: 0.2481, Accuracy: 1.0000\n",
      "Train Batch 115/175 - Loss: 0.4656, Accuracy: 0.8125\n",
      "Train Batch 116/175 - Loss: 0.5170, Accuracy: 0.8125\n",
      "Train Batch 117/175 - Loss: 0.5072, Accuracy: 0.8125\n",
      "Train Batch 118/175 - Loss: 0.2058, Accuracy: 0.9375\n",
      "Train Batch 119/175 - Loss: 0.5322, Accuracy: 0.9062\n",
      "Train Batch 120/175 - Loss: 0.8604, Accuracy: 0.7500\n",
      "Train Batch 121/175 - Loss: 0.5989, Accuracy: 0.8125\n",
      "Train Batch 122/175 - Loss: 0.1858, Accuracy: 0.9375\n",
      "Train Batch 123/175 - Loss: 0.4925, Accuracy: 0.8750\n",
      "Train Batch 124/175 - Loss: 0.2341, Accuracy: 0.9062\n",
      "Train Batch 125/175 - Loss: 0.2514, Accuracy: 0.9375\n",
      "Train Batch 126/175 - Loss: 0.4583, Accuracy: 0.8750\n",
      "Train Batch 127/175 - Loss: 1.0253, Accuracy: 0.8125\n",
      "Train Batch 128/175 - Loss: 0.5707, Accuracy: 0.8750\n",
      "Train Batch 129/175 - Loss: 0.5037, Accuracy: 0.8438\n",
      "Train Batch 130/175 - Loss: 0.1894, Accuracy: 0.9062\n",
      "Train Batch 131/175 - Loss: 0.4191, Accuracy: 0.8125\n",
      "Train Batch 132/175 - Loss: 0.4494, Accuracy: 0.8750\n",
      "Train Batch 133/175 - Loss: 0.1943, Accuracy: 0.9375\n",
      "Train Batch 134/175 - Loss: 0.5475, Accuracy: 0.8750\n",
      "Train Batch 135/175 - Loss: 0.2933, Accuracy: 0.9062\n",
      "Train Batch 136/175 - Loss: 0.5691, Accuracy: 0.8438\n",
      "Train Batch 137/175 - Loss: 0.1941, Accuracy: 1.0000\n",
      "Train Batch 138/175 - Loss: 0.4246, Accuracy: 0.9375\n",
      "Train Batch 139/175 - Loss: 0.1553, Accuracy: 1.0000\n",
      "Train Batch 140/175 - Loss: 0.5983, Accuracy: 0.8438\n",
      "Train Batch 141/175 - Loss: 0.4193, Accuracy: 0.9375\n",
      "Train Batch 142/175 - Loss: 0.4841, Accuracy: 0.8438\n",
      "Train Batch 143/175 - Loss: 0.5921, Accuracy: 0.8438\n",
      "Train Batch 144/175 - Loss: 0.9727, Accuracy: 0.8438\n",
      "Train Batch 145/175 - Loss: 0.2019, Accuracy: 0.9375\n",
      "Train Batch 146/175 - Loss: 0.5481, Accuracy: 0.8750\n",
      "Train Batch 147/175 - Loss: 0.3299, Accuracy: 0.9062\n",
      "Train Batch 148/175 - Loss: 0.4310, Accuracy: 0.8438\n",
      "Train Batch 149/175 - Loss: 0.4131, Accuracy: 0.9062\n",
      "Train Batch 150/175 - Loss: 0.3504, Accuracy: 0.9688\n",
      "Train Batch 151/175 - Loss: 0.3647, Accuracy: 0.8125\n",
      "Train Batch 152/175 - Loss: 0.4632, Accuracy: 0.8125\n",
      "Train Batch 153/175 - Loss: 0.3324, Accuracy: 0.9062\n",
      "Train Batch 154/175 - Loss: 0.3528, Accuracy: 0.8750\n",
      "Train Batch 155/175 - Loss: 0.3062, Accuracy: 0.9062\n",
      "Train Batch 156/175 - Loss: 0.2689, Accuracy: 0.8750\n",
      "Train Batch 157/175 - Loss: 0.2556, Accuracy: 0.9688\n",
      "Train Batch 158/175 - Loss: 0.4284, Accuracy: 0.8750\n",
      "Train Batch 159/175 - Loss: 0.3938, Accuracy: 0.9062\n",
      "Train Batch 160/175 - Loss: 0.1607, Accuracy: 1.0000\n",
      "Train Batch 161/175 - Loss: 0.6118, Accuracy: 0.7812\n",
      "Train Batch 162/175 - Loss: 0.1313, Accuracy: 0.9688\n",
      "Train Batch 163/175 - Loss: 0.1972, Accuracy: 0.9062\n",
      "Train Batch 164/175 - Loss: 0.3773, Accuracy: 0.9062\n",
      "Train Batch 165/175 - Loss: 0.4608, Accuracy: 0.8750\n",
      "Train Batch 166/175 - Loss: 0.5412, Accuracy: 0.8125\n",
      "Train Batch 167/175 - Loss: 0.2204, Accuracy: 0.9375\n",
      "Train Batch 168/175 - Loss: 0.0771, Accuracy: 0.9688\n",
      "Train Batch 169/175 - Loss: 0.4015, Accuracy: 0.9375\n",
      "Train Batch 170/175 - Loss: 0.3652, Accuracy: 0.9375\n",
      "Train Batch 171/175 - Loss: 0.2804, Accuracy: 0.9375\n",
      "Train Batch 172/175 - Loss: 0.3972, Accuracy: 0.8750\n",
      "Train Batch 173/175 - Loss: 0.3006, Accuracy: 0.9062\n",
      "Train Batch 174/175 - Loss: 0.6033, Accuracy: 0.7500\n",
      "Train Batch 175/175 - Loss: 0.2976, Accuracy: 0.9474\n",
      "Train Epoch Loss: 0.4325 Acc: 0.8806\n",
      "Val Batch 1/44 - Loss: 0.1356, Accuracy: 0.9688\n",
      "Val Batch 2/44 - Loss: 0.2540, Accuracy: 0.9375\n",
      "Val Batch 3/44 - Loss: 0.1031, Accuracy: 0.9688\n",
      "Val Batch 4/44 - Loss: 0.5049, Accuracy: 0.8438\n",
      "Val Batch 5/44 - Loss: 0.1411, Accuracy: 0.9688\n",
      "Val Batch 6/44 - Loss: 0.5278, Accuracy: 0.8750\n",
      "Val Batch 7/44 - Loss: 0.4479, Accuracy: 0.8750\n",
      "Val Batch 8/44 - Loss: 0.3962, Accuracy: 0.8438\n",
      "Val Batch 9/44 - Loss: 0.1666, Accuracy: 0.9375\n",
      "Val Batch 10/44 - Loss: 0.3282, Accuracy: 0.9062\n",
      "Val Batch 11/44 - Loss: 0.2528, Accuracy: 0.9062\n",
      "Val Batch 12/44 - Loss: 0.6183, Accuracy: 0.8750\n",
      "Val Batch 13/44 - Loss: 0.0718, Accuracy: 0.9688\n",
      "Val Batch 14/44 - Loss: 0.2369, Accuracy: 0.9688\n",
      "Val Batch 15/44 - Loss: 0.3522, Accuracy: 0.8750\n",
      "Val Batch 16/44 - Loss: 0.6241, Accuracy: 0.8125\n",
      "Val Batch 17/44 - Loss: 0.2877, Accuracy: 0.9062\n",
      "Val Batch 18/44 - Loss: 0.2516, Accuracy: 0.8750\n",
      "Val Batch 19/44 - Loss: 0.4943, Accuracy: 0.8125\n",
      "Val Batch 20/44 - Loss: 0.2606, Accuracy: 0.9062\n",
      "Val Batch 21/44 - Loss: 0.2011, Accuracy: 0.9062\n",
      "Val Batch 22/44 - Loss: 0.6332, Accuracy: 0.8438\n",
      "Val Batch 23/44 - Loss: 0.5138, Accuracy: 0.8438\n",
      "Val Batch 24/44 - Loss: 0.7072, Accuracy: 0.8438\n",
      "Val Batch 25/44 - Loss: 0.3563, Accuracy: 0.9375\n",
      "Val Batch 26/44 - Loss: 0.2410, Accuracy: 0.9062\n",
      "Val Batch 27/44 - Loss: 0.1610, Accuracy: 0.9688\n",
      "Val Batch 28/44 - Loss: 0.3837, Accuracy: 0.9375\n",
      "Val Batch 29/44 - Loss: 0.4074, Accuracy: 0.8750\n",
      "Val Batch 30/44 - Loss: 0.0853, Accuracy: 0.9688\n",
      "Val Batch 31/44 - Loss: 0.2353, Accuracy: 0.9375\n",
      "Val Batch 32/44 - Loss: 0.1591, Accuracy: 0.9688\n",
      "Val Batch 33/44 - Loss: 0.1490, Accuracy: 0.9688\n",
      "Val Batch 34/44 - Loss: 0.0725, Accuracy: 1.0000\n",
      "Val Batch 35/44 - Loss: 0.4475, Accuracy: 0.8750\n",
      "Val Batch 36/44 - Loss: 0.2363, Accuracy: 0.9688\n",
      "Val Batch 37/44 - Loss: 0.8134, Accuracy: 0.7500\n",
      "Val Batch 38/44 - Loss: 0.2832, Accuracy: 0.9375\n",
      "Val Batch 39/44 - Loss: 0.1449, Accuracy: 0.9688\n",
      "Val Batch 40/44 - Loss: 0.4011, Accuracy: 0.9062\n",
      "Val Batch 41/44 - Loss: 0.6010, Accuracy: 0.8438\n",
      "Val Batch 42/44 - Loss: 0.3131, Accuracy: 0.8750\n",
      "Val Batch 43/44 - Loss: 0.2685, Accuracy: 0.8750\n",
      "Val Batch 44/44 - Loss: 0.2402, Accuracy: 0.9231\n",
      "Val Epoch Loss: 0.3302 Acc: 0.9058\n",
      "Epoch 5/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 0.1056, Accuracy: 0.9688\n",
      "Train Batch 2/175 - Loss: 0.4098, Accuracy: 0.8750\n",
      "Train Batch 3/175 - Loss: 0.3301, Accuracy: 0.9375\n",
      "Train Batch 4/175 - Loss: 0.1327, Accuracy: 0.9688\n",
      "Train Batch 5/175 - Loss: 0.1283, Accuracy: 0.9375\n",
      "Train Batch 6/175 - Loss: 0.1863, Accuracy: 0.9375\n",
      "Train Batch 7/175 - Loss: 0.3433, Accuracy: 0.9062\n",
      "Train Batch 8/175 - Loss: 0.4460, Accuracy: 0.9062\n",
      "Train Batch 9/175 - Loss: 0.1851, Accuracy: 0.9375\n",
      "Train Batch 10/175 - Loss: 0.1572, Accuracy: 0.9688\n",
      "Train Batch 11/175 - Loss: 0.1689, Accuracy: 0.9375\n",
      "Train Batch 12/175 - Loss: 0.1202, Accuracy: 0.9688\n",
      "Train Batch 13/175 - Loss: 0.1255, Accuracy: 0.9688\n",
      "Train Batch 14/175 - Loss: 0.1297, Accuracy: 0.9688\n",
      "Train Batch 15/175 - Loss: 0.1732, Accuracy: 0.9688\n",
      "Train Batch 16/175 - Loss: 0.1169, Accuracy: 1.0000\n",
      "Train Batch 17/175 - Loss: 0.2026, Accuracy: 0.9375\n",
      "Train Batch 18/175 - Loss: 0.3113, Accuracy: 0.9375\n",
      "Train Batch 19/175 - Loss: 0.3491, Accuracy: 0.9062\n",
      "Train Batch 20/175 - Loss: 0.1688, Accuracy: 1.0000\n",
      "Train Batch 21/175 - Loss: 0.2377, Accuracy: 0.9375\n",
      "Train Batch 22/175 - Loss: 0.4830, Accuracy: 0.8438\n",
      "Train Batch 23/175 - Loss: 0.3553, Accuracy: 0.8750\n",
      "Train Batch 24/175 - Loss: 0.1515, Accuracy: 0.9375\n",
      "Train Batch 25/175 - Loss: 0.2302, Accuracy: 0.9375\n",
      "Train Batch 26/175 - Loss: 0.4032, Accuracy: 0.9375\n",
      "Train Batch 27/175 - Loss: 0.1615, Accuracy: 0.9375\n",
      "Train Batch 28/175 - Loss: 0.1108, Accuracy: 0.9688\n",
      "Train Batch 29/175 - Loss: 0.1395, Accuracy: 0.9688\n",
      "Train Batch 30/175 - Loss: 0.1592, Accuracy: 0.9688\n",
      "Train Batch 31/175 - Loss: 0.1097, Accuracy: 1.0000\n",
      "Train Batch 32/175 - Loss: 0.2678, Accuracy: 0.9062\n",
      "Train Batch 33/175 - Loss: 0.3869, Accuracy: 0.8438\n",
      "Train Batch 34/175 - Loss: 0.3457, Accuracy: 0.8750\n",
      "Train Batch 35/175 - Loss: 0.4262, Accuracy: 0.8438\n",
      "Train Batch 36/175 - Loss: 0.3322, Accuracy: 0.9062\n",
      "Train Batch 37/175 - Loss: 0.4556, Accuracy: 0.9062\n",
      "Train Batch 38/175 - Loss: 0.1875, Accuracy: 0.9688\n",
      "Train Batch 39/175 - Loss: 0.0847, Accuracy: 1.0000\n",
      "Train Batch 40/175 - Loss: 0.2166, Accuracy: 0.9688\n",
      "Train Batch 41/175 - Loss: 0.3613, Accuracy: 0.9375\n",
      "Train Batch 42/175 - Loss: 0.5075, Accuracy: 0.8750\n",
      "Train Batch 43/175 - Loss: 0.7261, Accuracy: 0.8438\n",
      "Train Batch 44/175 - Loss: 0.3122, Accuracy: 0.9375\n",
      "Train Batch 45/175 - Loss: 0.2858, Accuracy: 0.9375\n",
      "Train Batch 46/175 - Loss: 0.0870, Accuracy: 1.0000\n",
      "Train Batch 47/175 - Loss: 0.3066, Accuracy: 0.9062\n",
      "Train Batch 48/175 - Loss: 0.2056, Accuracy: 0.9375\n",
      "Train Batch 49/175 - Loss: 0.3705, Accuracy: 0.8438\n",
      "Train Batch 50/175 - Loss: 0.3655, Accuracy: 0.8750\n",
      "Train Batch 51/175 - Loss: 0.2608, Accuracy: 0.9375\n",
      "Train Batch 52/175 - Loss: 0.1977, Accuracy: 0.9375\n",
      "Train Batch 53/175 - Loss: 0.2539, Accuracy: 0.9688\n",
      "Train Batch 54/175 - Loss: 0.3381, Accuracy: 0.8438\n",
      "Train Batch 55/175 - Loss: 0.3396, Accuracy: 0.8438\n",
      "Train Batch 56/175 - Loss: 0.1967, Accuracy: 0.9688\n",
      "Train Batch 57/175 - Loss: 0.5102, Accuracy: 0.8438\n",
      "Train Batch 58/175 - Loss: 0.1605, Accuracy: 0.9375\n",
      "Train Batch 59/175 - Loss: 0.2132, Accuracy: 0.9375\n",
      "Train Batch 60/175 - Loss: 0.5057, Accuracy: 0.9062\n",
      "Train Batch 61/175 - Loss: 0.3817, Accuracy: 0.8125\n",
      "Train Batch 62/175 - Loss: 0.1410, Accuracy: 0.9688\n",
      "Train Batch 63/175 - Loss: 0.3711, Accuracy: 0.9062\n",
      "Train Batch 64/175 - Loss: 0.1588, Accuracy: 0.9375\n",
      "Train Batch 65/175 - Loss: 0.3547, Accuracy: 0.9062\n",
      "Train Batch 66/175 - Loss: 0.2925, Accuracy: 0.9062\n",
      "Train Batch 67/175 - Loss: 0.2448, Accuracy: 0.9375\n",
      "Train Batch 68/175 - Loss: 0.3335, Accuracy: 0.8750\n",
      "Train Batch 69/175 - Loss: 0.4832, Accuracy: 0.8750\n",
      "Train Batch 70/175 - Loss: 0.2153, Accuracy: 0.9062\n",
      "Train Batch 71/175 - Loss: 0.1981, Accuracy: 0.9062\n",
      "Train Batch 72/175 - Loss: 0.0413, Accuracy: 1.0000\n",
      "Train Batch 73/175 - Loss: 0.2438, Accuracy: 0.9375\n",
      "Train Batch 74/175 - Loss: 0.1024, Accuracy: 0.9688\n",
      "Train Batch 75/175 - Loss: 0.7542, Accuracy: 0.8750\n",
      "Train Batch 76/175 - Loss: 0.0524, Accuracy: 0.9688\n",
      "Train Batch 77/175 - Loss: 0.2836, Accuracy: 0.9688\n",
      "Train Batch 78/175 - Loss: 0.2071, Accuracy: 0.9062\n",
      "Train Batch 79/175 - Loss: 0.3445, Accuracy: 0.9062\n",
      "Train Batch 80/175 - Loss: 0.3706, Accuracy: 0.8750\n",
      "Train Batch 81/175 - Loss: 0.1788, Accuracy: 0.9688\n",
      "Train Batch 82/175 - Loss: 0.1744, Accuracy: 0.9688\n",
      "Train Batch 83/175 - Loss: 0.1431, Accuracy: 0.9688\n",
      "Train Batch 84/175 - Loss: 0.2585, Accuracy: 0.9688\n",
      "Train Batch 85/175 - Loss: 0.2351, Accuracy: 0.9375\n",
      "Train Batch 86/175 - Loss: 0.1020, Accuracy: 1.0000\n",
      "Train Batch 87/175 - Loss: 0.2800, Accuracy: 0.9062\n",
      "Train Batch 88/175 - Loss: 0.2169, Accuracy: 0.9375\n",
      "Train Batch 89/175 - Loss: 0.2498, Accuracy: 0.9375\n",
      "Train Batch 90/175 - Loss: 0.1324, Accuracy: 0.9688\n",
      "Train Batch 91/175 - Loss: 0.2623, Accuracy: 0.9062\n",
      "Train Batch 92/175 - Loss: 0.2455, Accuracy: 0.8750\n",
      "Train Batch 93/175 - Loss: 0.0823, Accuracy: 1.0000\n",
      "Train Batch 94/175 - Loss: 0.0858, Accuracy: 1.0000\n",
      "Train Batch 95/175 - Loss: 0.1543, Accuracy: 0.9375\n",
      "Train Batch 96/175 - Loss: 0.3027, Accuracy: 0.8438\n",
      "Train Batch 97/175 - Loss: 0.2858, Accuracy: 0.9062\n",
      "Train Batch 98/175 - Loss: 0.0708, Accuracy: 0.9688\n",
      "Train Batch 99/175 - Loss: 0.4659, Accuracy: 0.9688\n",
      "Train Batch 100/175 - Loss: 0.1143, Accuracy: 0.9375\n",
      "Train Batch 101/175 - Loss: 0.1447, Accuracy: 0.9375\n",
      "Train Batch 102/175 - Loss: 0.1577, Accuracy: 0.9688\n",
      "Train Batch 103/175 - Loss: 0.3196, Accuracy: 0.9375\n",
      "Train Batch 104/175 - Loss: 0.1037, Accuracy: 0.9688\n",
      "Train Batch 105/175 - Loss: 0.4673, Accuracy: 0.8125\n",
      "Train Batch 106/175 - Loss: 0.1366, Accuracy: 0.9062\n",
      "Train Batch 107/175 - Loss: 0.2607, Accuracy: 0.9062\n",
      "Train Batch 108/175 - Loss: 0.0905, Accuracy: 0.9688\n",
      "Train Batch 109/175 - Loss: 0.1016, Accuracy: 0.9688\n",
      "Train Batch 110/175 - Loss: 0.2806, Accuracy: 0.9688\n",
      "Train Batch 111/175 - Loss: 0.2806, Accuracy: 0.9375\n",
      "Train Batch 112/175 - Loss: 0.4973, Accuracy: 0.8750\n",
      "Train Batch 113/175 - Loss: 0.1365, Accuracy: 0.9688\n",
      "Train Batch 114/175 - Loss: 0.1559, Accuracy: 0.9688\n",
      "Train Batch 115/175 - Loss: 0.1403, Accuracy: 0.9375\n",
      "Train Batch 116/175 - Loss: 0.4721, Accuracy: 0.8750\n",
      "Train Batch 117/175 - Loss: 0.0601, Accuracy: 1.0000\n",
      "Train Batch 118/175 - Loss: 0.3062, Accuracy: 0.9062\n",
      "Train Batch 119/175 - Loss: 0.5305, Accuracy: 0.8750\n",
      "Train Batch 120/175 - Loss: 0.2719, Accuracy: 0.9062\n",
      "Train Batch 121/175 - Loss: 0.0922, Accuracy: 1.0000\n",
      "Train Batch 122/175 - Loss: 0.1407, Accuracy: 0.9375\n",
      "Train Batch 123/175 - Loss: 0.2139, Accuracy: 0.9375\n",
      "Train Batch 124/175 - Loss: 0.3674, Accuracy: 0.9062\n",
      "Train Batch 125/175 - Loss: 0.2153, Accuracy: 0.9375\n",
      "Train Batch 126/175 - Loss: 0.3098, Accuracy: 0.9062\n",
      "Train Batch 127/175 - Loss: 0.4999, Accuracy: 0.8750\n",
      "Train Batch 128/175 - Loss: 0.2733, Accuracy: 0.8750\n",
      "Train Batch 129/175 - Loss: 0.3229, Accuracy: 0.8750\n",
      "Train Batch 130/175 - Loss: 0.2147, Accuracy: 0.9062\n",
      "Train Batch 131/175 - Loss: 0.2751, Accuracy: 0.9375\n",
      "Train Batch 132/175 - Loss: 0.1362, Accuracy: 0.9688\n",
      "Train Batch 133/175 - Loss: 0.3578, Accuracy: 0.8750\n",
      "Train Batch 134/175 - Loss: 0.4050, Accuracy: 0.9375\n",
      "Train Batch 135/175 - Loss: 0.3053, Accuracy: 0.8750\n",
      "Train Batch 136/175 - Loss: 0.2710, Accuracy: 0.9375\n",
      "Train Batch 137/175 - Loss: 0.1699, Accuracy: 0.9375\n",
      "Train Batch 138/175 - Loss: 0.1814, Accuracy: 0.9375\n",
      "Train Batch 139/175 - Loss: 0.4964, Accuracy: 0.9375\n",
      "Train Batch 140/175 - Loss: 0.3165, Accuracy: 0.9062\n",
      "Train Batch 141/175 - Loss: 0.1244, Accuracy: 1.0000\n",
      "Train Batch 142/175 - Loss: 0.0849, Accuracy: 0.9688\n",
      "Train Batch 143/175 - Loss: 0.3324, Accuracy: 0.9375\n",
      "Train Batch 144/175 - Loss: 0.1784, Accuracy: 0.9375\n",
      "Train Batch 145/175 - Loss: 0.2792, Accuracy: 0.9062\n",
      "Train Batch 146/175 - Loss: 0.5482, Accuracy: 0.8438\n",
      "Train Batch 147/175 - Loss: 0.1036, Accuracy: 1.0000\n",
      "Train Batch 148/175 - Loss: 0.0514, Accuracy: 1.0000\n",
      "Train Batch 149/175 - Loss: 0.2339, Accuracy: 0.9062\n",
      "Train Batch 150/175 - Loss: 0.4224, Accuracy: 0.8750\n",
      "Train Batch 151/175 - Loss: 0.2406, Accuracy: 0.9375\n",
      "Train Batch 152/175 - Loss: 0.2896, Accuracy: 0.9375\n",
      "Train Batch 153/175 - Loss: 0.3003, Accuracy: 0.9062\n",
      "Train Batch 154/175 - Loss: 0.1430, Accuracy: 0.9688\n",
      "Train Batch 155/175 - Loss: 0.0923, Accuracy: 0.9688\n",
      "Train Batch 156/175 - Loss: 0.1496, Accuracy: 0.9375\n",
      "Train Batch 157/175 - Loss: 0.0497, Accuracy: 1.0000\n",
      "Train Batch 158/175 - Loss: 0.1158, Accuracy: 0.9688\n",
      "Train Batch 159/175 - Loss: 0.3365, Accuracy: 0.9375\n",
      "Train Batch 160/175 - Loss: 0.5044, Accuracy: 0.9062\n",
      "Train Batch 161/175 - Loss: 0.4554, Accuracy: 0.9375\n",
      "Train Batch 162/175 - Loss: 0.1853, Accuracy: 0.9375\n",
      "Train Batch 163/175 - Loss: 0.0104, Accuracy: 1.0000\n",
      "Train Batch 164/175 - Loss: 0.1615, Accuracy: 0.9375\n",
      "Train Batch 165/175 - Loss: 0.1820, Accuracy: 0.9375\n",
      "Train Batch 166/175 - Loss: 0.1605, Accuracy: 0.9062\n",
      "Train Batch 167/175 - Loss: 0.3346, Accuracy: 0.9375\n",
      "Train Batch 168/175 - Loss: 0.2141, Accuracy: 0.9375\n",
      "Train Batch 169/175 - Loss: 0.1674, Accuracy: 0.9375\n",
      "Train Batch 170/175 - Loss: 0.4434, Accuracy: 0.8750\n",
      "Train Batch 171/175 - Loss: 0.5034, Accuracy: 0.8750\n",
      "Train Batch 172/175 - Loss: 0.1933, Accuracy: 0.9688\n",
      "Train Batch 173/175 - Loss: 0.3135, Accuracy: 0.9062\n",
      "Train Batch 174/175 - Loss: 0.0969, Accuracy: 1.0000\n",
      "Train Batch 175/175 - Loss: 0.0906, Accuracy: 1.0000\n",
      "Train Epoch Loss: 0.2531 Acc: 0.9306\n",
      "Val Batch 1/44 - Loss: 0.2113, Accuracy: 0.9375\n",
      "Val Batch 2/44 - Loss: 0.3205, Accuracy: 0.9375\n",
      "Val Batch 3/44 - Loss: 0.0264, Accuracy: 1.0000\n",
      "Val Batch 4/44 - Loss: 0.4698, Accuracy: 0.9062\n",
      "Val Batch 5/44 - Loss: 0.1727, Accuracy: 0.9688\n",
      "Val Batch 6/44 - Loss: 0.6907, Accuracy: 0.8125\n",
      "Val Batch 7/44 - Loss: 0.3754, Accuracy: 0.8750\n",
      "Val Batch 8/44 - Loss: 0.2938, Accuracy: 0.9375\n",
      "Val Batch 9/44 - Loss: 0.1482, Accuracy: 1.0000\n",
      "Val Batch 10/44 - Loss: 0.3343, Accuracy: 0.9062\n",
      "Val Batch 11/44 - Loss: 0.2020, Accuracy: 0.9375\n",
      "Val Batch 12/44 - Loss: 0.4788, Accuracy: 0.8438\n",
      "Val Batch 13/44 - Loss: 0.0350, Accuracy: 1.0000\n",
      "Val Batch 14/44 - Loss: 0.1381, Accuracy: 0.9375\n",
      "Val Batch 15/44 - Loss: 0.0723, Accuracy: 0.9688\n",
      "Val Batch 16/44 - Loss: 0.3485, Accuracy: 0.9375\n",
      "Val Batch 17/44 - Loss: 0.2468, Accuracy: 0.9062\n",
      "Val Batch 18/44 - Loss: 0.5950, Accuracy: 0.8125\n",
      "Val Batch 19/44 - Loss: 0.5744, Accuracy: 0.7812\n",
      "Val Batch 20/44 - Loss: 0.2002, Accuracy: 0.9062\n",
      "Val Batch 21/44 - Loss: 0.1690, Accuracy: 0.9375\n",
      "Val Batch 22/44 - Loss: 0.6248, Accuracy: 0.8750\n",
      "Val Batch 23/44 - Loss: 0.1965, Accuracy: 0.9688\n",
      "Val Batch 24/44 - Loss: 0.3176, Accuracy: 0.9375\n",
      "Val Batch 25/44 - Loss: 0.4013, Accuracy: 0.9375\n",
      "Val Batch 26/44 - Loss: 0.1483, Accuracy: 0.9688\n",
      "Val Batch 27/44 - Loss: 0.0727, Accuracy: 1.0000\n",
      "Val Batch 28/44 - Loss: 0.3765, Accuracy: 0.9062\n",
      "Val Batch 29/44 - Loss: 0.2889, Accuracy: 0.8750\n",
      "Val Batch 30/44 - Loss: 0.1209, Accuracy: 0.9688\n",
      "Val Batch 31/44 - Loss: 0.2621, Accuracy: 0.9062\n",
      "Val Batch 32/44 - Loss: 0.1342, Accuracy: 0.9688\n",
      "Val Batch 33/44 - Loss: 0.2104, Accuracy: 0.9375\n",
      "Val Batch 34/44 - Loss: 0.1042, Accuracy: 1.0000\n",
      "Val Batch 35/44 - Loss: 0.4916, Accuracy: 0.8438\n",
      "Val Batch 36/44 - Loss: 0.2174, Accuracy: 0.9375\n",
      "Val Batch 37/44 - Loss: 0.7582, Accuracy: 0.7812\n",
      "Val Batch 38/44 - Loss: 0.2953, Accuracy: 0.9375\n",
      "Val Batch 39/44 - Loss: 0.3523, Accuracy: 0.8750\n",
      "Val Batch 40/44 - Loss: 0.4258, Accuracy: 0.8438\n",
      "Val Batch 41/44 - Loss: 0.5333, Accuracy: 0.9062\n",
      "Val Batch 42/44 - Loss: 0.1824, Accuracy: 0.9062\n",
      "Val Batch 43/44 - Loss: 0.0848, Accuracy: 1.0000\n",
      "Val Batch 44/44 - Loss: 0.1659, Accuracy: 0.9231\n",
      "Val Epoch Loss: 0.2930 Acc: 0.9194\n",
      "Epoch 6/10\n",
      "--------------------\n",
      "Train Batch 1/175 - Loss: 0.1394, Accuracy: 0.9375\n",
      "Train Batch 2/175 - Loss: 0.1269, Accuracy: 1.0000\n",
      "Train Batch 3/175 - Loss: 0.2127, Accuracy: 0.8750\n",
      "Train Batch 4/175 - Loss: 0.0713, Accuracy: 1.0000\n",
      "Train Batch 5/175 - Loss: 0.0633, Accuracy: 1.0000\n",
      "Train Batch 6/175 - Loss: 0.1299, Accuracy: 0.9375\n",
      "Train Batch 7/175 - Loss: 0.0766, Accuracy: 0.9688\n",
      "Train Batch 8/175 - Loss: 0.1418, Accuracy: 0.9375\n",
      "Train Batch 9/175 - Loss: 0.3413, Accuracy: 0.8438\n",
      "Train Batch 10/175 - Loss: 0.0939, Accuracy: 0.9688\n",
      "Train Batch 11/175 - Loss: 0.0941, Accuracy: 1.0000\n",
      "Train Batch 12/175 - Loss: 0.0922, Accuracy: 0.9688\n",
      "Train Batch 13/175 - Loss: 0.2692, Accuracy: 0.8750\n",
      "Train Batch 14/175 - Loss: 0.1261, Accuracy: 0.9688\n",
      "Train Batch 15/175 - Loss: 0.0921, Accuracy: 0.9688\n",
      "Train Batch 16/175 - Loss: 0.0981, Accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 137\u001b[0m\n\u001b[1;32m    130\u001b[0m hyperparameter_configs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m27\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m},\n\u001b[1;32m    132\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m27\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0005\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m64\u001b[39m},\n\u001b[1;32m    133\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m27\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m16\u001b[39m},\n\u001b[1;32m    134\u001b[0m ]\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Run training and plot results\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_plot_hyperparameter_effects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameter_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Display summary results\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mtrain_and_plot_hyperparameter_effects\u001b[0;34m(hyperparameters, image_datasets, num_epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     69\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def train_and_plot_hyperparameter_effects(hyperparameters, image_datasets, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the model with different hyperparameter combinations and plots results.\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (list of dict): List of hyperparameter dictionaries.\n",
    "        image_datasets (dict): Datasets for 'train' and 'val' datasets.\n",
    "        num_epochs (int): Number of epochs for each training run.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with hyperparameter configurations and corresponding results.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = []\n",
    "\n",
    "    for config in hyperparameters:\n",
    "        print(f\"Training with hyperparameters: {config}\")\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        num_classes = config.get('num_classes', 27)\n",
    "        learning_rate = config.get('learning_rate', 0.001)\n",
    "        batch_size = config.get('batch_size', 32)\n",
    "\n",
    "        # Initialize model, criterion, optimizer\n",
    "        model = ASLClassifierCNN(num_classes=num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Update dataloaders for the current batch size\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "            'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        }\n",
    "\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print('-' * 20)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                total_samples = 0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "\n",
    "                for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    total_samples += labels.size(0)\n",
    "\n",
    "                    batch_loss = loss.item()\n",
    "                    batch_acc = torch.sum(preds == labels.data).double() / labels.size(0)\n",
    "\n",
    "                    print(f\"{phase.capitalize()} Batch {i+1}/{len(dataloaders[phase])} - Loss: {batch_loss:.4f}, Accuracy: {batch_acc:.4f}\")\n",
    "\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                epoch_loss = running_loss / len(image_datasets[phase])\n",
    "                epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "                if phase == 'train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_accs.append(epoch_acc.item())\n",
    "                else:\n",
    "                    val_losses.append(epoch_loss)\n",
    "                    val_accs.append(epoch_acc.item())\n",
    "\n",
    "                    # Check if the model improved\n",
    "                    if epoch_acc > best_val_acc:\n",
    "                        best_val_acc = epoch_acc\n",
    "                        torch.save(model.state_dict(), 'best_model.pth')\n",
    "                        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "\n",
    "                print(f\"{phase.capitalize()} Epoch Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        # Save results for this configuration\n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'train_loss': train_losses[-1],\n",
    "            'val_loss': val_losses[-1],\n",
    "            'train_acc': train_accs[-1],\n",
    "            'val_acc': val_accs[-1]\n",
    "        })\n",
    "\n",
    "        # Plot learning curves for this configuration\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss Curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f\"Loss Curve ({config})\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy Curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, num_epochs + 1), train_accs, label='Train Accuracy')\n",
    "        plt.plot(range(1, num_epochs + 1), val_accs, label='Val Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f\"Accuracy Curve ({config})\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot confusion matrix for validation set\n",
    "        if len(all_preds) > 0 and len(all_labels) > 0:\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.title(f'Confusion Matrix ({config})')\n",
    "            plt.show()\n",
    "\n",
    "    # Convert results to a DataFrame for further analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"All configurations completed!\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example of hyperparameter configurations to test\n",
    "hyperparameter_configs = [\n",
    "    {'num_classes': 27, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "    {'num_classes': 27, 'learning_rate': 0.0005, 'batch_size': 64},\n",
    "    {'num_classes': 27, 'learning_rate': 0.005, 'batch_size': 16},\n",
    "]\n",
    "\n",
    "# Run training and plot results\n",
    "results_df = train_and_plot_hyperparameter_effects(hyperparameter_configs, image_datasets, num_epochs=10)\n",
    "\n",
    "# Display summary results\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
